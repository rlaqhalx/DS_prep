# Decision Tree

# ğŸ“Œ 1. Classification and Regression Trees (CART)

Decision Trees are used for both classification and regression tasks. They work by splitting the dataset into smaller regions based on feature values.

ğŸ”¹ How **CART** Works?

### For Regression:

- Splits data to minimize Sum of Squared Errors (SSE). ì¦‰, ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ë©´ì„œ SSE(ì˜¤ì°¨ ì œê³±í•©)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë‚˜ëˆ”

- Each leaf node represents the average of the target values in that region. ì¦‰, ë§ˆì§€ë§‰ ë…¸ë“œ(Leaf Node)ì—ì„œ í‰ê· ê°’ì„ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©

- Complexity Parameter (cp) controls tree depth (small cp â†’ deep tree).

ğŸ“Œ ì˜ˆì œ: ì§‘ê°’ ì˜ˆì¸¡

âœ” ë©´ì  > 50ã¡ â†’ ì§‘ê°’ í‰ê·  5000ë§Œ ì›

âœ” ë©´ì  â‰¤ 50ã¡ â†’ ì§‘ê°’ í‰ê·  3000ë§Œ ì›

âœ… ì¤‘ìš”í•œ ê°œë…:

âœ” SSE(Sum of Squared Errors, ì˜¤ì°¨ ì œê³±í•©) ìµœì†Œí™”

âœ” ê¹Šì€ íŠ¸ë¦¬(Deep Tree)ëŠ” ê³¼ì í•©(Overfitting) ìœ„í—˜ ìˆìŒ â†’ ì´ë¥¼ ë°©ì§€í•˜ë ¤ë©´ Pruning(ê°€ì§€ì¹˜ê¸°)ë‚˜ Complexity Parameter(cp) ì‚¬ìš©

### For Classification:

- Splits data to minimize impurity in each region. ì¦‰, ë°ì´í„°ë¥¼ ë‚˜ëˆ„ë©´ì„œ ë¶ˆìˆœë„(Impurity) ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë‚˜ëˆ”

- Each leaf node assigns the most frequent class label. ë§ˆì°¬ê°€ì§€ë¡œ ë§ˆì§€ë§‰ ë…¸ë“œ(Leaf Node)ì—ì„œ ê°€ì¥ ë§ì€ í´ë˜ìŠ¤ë¥¼ ì •ë‹µìœ¼ë¡œ ì˜ˆì¸¡

ğŸ“Œ ì˜ˆì œ: ì´ë©”ì¼ ìŠ¤íŒ¸ ë¶„ë¥˜

âœ” ì œëª©ì— "Free" í¬í•¨ â†’ ìŠ¤íŒ¸ í™•ë¥  80%

âœ” ì œëª©ì— "Urgent" í¬í•¨ â†’ ìŠ¤íŒ¸ í™•ë¥  70%

### Common impurity measures:

**Gini Impurity**: Measures the probability of misclassification.

Gini = 1 - Î£(pÌ‚áµ¢Â²)

- ë°ì´í„°ê°€ íŠ¹ì • í´ë˜ìŠ¤ë¡œ ì–¼ë§ˆë‚˜ ê· ì¼í•˜ê²Œ ë¶„í¬ë˜ì–´ ìˆëŠ”ì§€ ì¸¡ì •

- ê°’ì´ ì‘ì„ìˆ˜ë¡ ë” ìˆœìˆ˜í•œ ê·¸ë£¹ (ì¦‰, ê°™ì€ í´ë˜ìŠ¤ê°€ ë§ìŒ)

**Cross Entropy (Log Loss)**: Measures information gain from each split.

Cross Entropy = -Î£(pÌ‚áµ¢ logâ‚‚ pÌ‚áµ¢)

- ë°ì´í„°ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •

- ê°’ì´ ì‘ì„ìˆ˜ë¡ ë” ì˜ˆì¸¡í•˜ê¸° ì‰¬ìš´ ë°ì´í„°


ğŸ“Œ ë¹„êµ: ì§€ë‹ˆ ë¶ˆìˆœë„ vs ì—”íŠ¸ë¡œí”¼

âœ” Gini ë¶ˆìˆœë„ â†’ ì—°ì‚° ì†ë„ê°€ ë¹ ë¦„

âœ” Entropy â†’ ì •ë³´ì´ë¡  ê¸°ë°˜, ì‘ì€ ë°ì´í„°ì—ì„œ ì¡°ê¸ˆ ë” ì•ˆì •ì 


### Decision Tree ê°€ ì˜í•˜ëŠ” ê²ƒ vs ì–´ë ¤ìš´ ê²ƒ

âœ” ì¥ì :

âœ… í•´ì„í•˜ê¸° ì‰½ë‹¤ â†’ íŠ¸ë¦¬ êµ¬ì¡°ë§Œ ë³´ë©´ ì–´ë–»ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥

âœ… ì´ìƒì¹˜(Outlier)ì™€ ë‹¤ì¤‘ê³µì„ ì„±(Multicollinearity)ì— ê°•í•¨ â†’ íŠ¸ë¦¬ê°€ ìŠ¤ìŠ¤ë¡œ ì¤‘ìš”í•œ ë³€ìˆ˜ë¥¼ ì„ íƒ 

(Multicollinearity & Outliers: Trees naturally handle them.)

âŒ ë‹¨ì :

âš  ê³¼ì í•©(Overfitting) ìœ„í—˜ â†’ íŠ¸ë¦¬ê°€ ë„ˆë¬´ ê¹Šì–´ì§€ë©´ í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì˜ ë§ì§€ë§Œ ìƒˆë¡œìš´ ë°ì´í„°ì—ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§

âš  ì‘ì€ ë³€í™”ì—ë„ ë¯¼ê°í•¨ â†’ ë°ì´í„°ê°€ ì¡°ê¸ˆë§Œ ë³€í•´ë„ íŠ¸ë¦¬ êµ¬ì¡°ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ 

(High Variance (Overfitting): Controlled using Cross Validation (CV) and pruning.)


# ğŸ“Œ 2. Random Forest (ëœë¤ í¬ë ˆìŠ¤íŠ¸)

An ensemble of multiple decision trees, reducing overfitting and improving accuracy.

ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ê°œì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ ì¡°í•©í•´ ë” ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

ğŸ“Œ ì™œ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í• ê¹Œ?

âœ” ì˜ì‚¬ê²°ì •ë‚˜ë¬´ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ë©´ ê³¼ì í•©ë  ê°€ëŠ¥ì„±ì´ í¼

âœ” ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ê°œì˜ ë‚˜ë¬´ë¥¼ í‰ê· ë‚´ì„œ ì˜ˆì¸¡ â†’ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì„!


ğŸ”¹ How Random Forest Works?

### 1. Bootstrapping: Randomly selects samples with replacement to train each tree.

í›ˆë ¨ ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ë½‘ì•„ì„œ ì—¬ëŸ¬ ê°œì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ í›ˆë ¨

ì¦‰, ê° ë‚˜ë¬´ëŠ” ë‹¤ë¥¸ ë°ì´í„° ìƒ˜í”Œì„ í•™ìŠµí•¨ â†’ ë‹¤ì–‘ì„±ì„ í™•ë³´!

### 2. Bagging (Bootstrap Aggregation):

Each tree is trained on ~63% of the data.

The remaining 37% (out-of-bag data) helps estimate model performance without cross-validation.

ê° íŠ¸ë¦¬(ë‚˜ë¬´)ëŠ” ëœë¤í•˜ê²Œ ìƒ˜í”Œë§ëœ ë°ì´í„°ë¡œ í›ˆë ¨ë¨

ê²°ê³¼ë¥¼ í‰ê·  ë‚´ê±°ë‚˜ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¥¼ í•´ì„œ ìµœì¢… ì˜ˆì¸¡ ê²°ì •!


![image](https://github.com/user-attachments/assets/7550b96d-371c-4136-a70e-d5fa0d832173)


Final Prediction:

Regression: Averages predictions from all trees.

Classification: Majority voting from all trees.

âœ… Example:

Predicting whether a customer will buy a product based on past purchases.

Predicting loan approval based on credit score, income, and debt.

### Random Forest ê°€ ì˜í•˜ëŠ” ê²ƒ vs ì–´ë ¤ìš´ ê²ƒ

âœ” ì¥ì :

âœ… ë¹„ì„ í˜• ë°ì´í„°(Non-linearity)ë„ ì˜ ì²˜ë¦¬

âœ… ê³ ì°¨ì› ë°ì´í„°(High-Dimensional Data)ì—ë„ ê°•í•¨

âœ… ê³¼ì í•©(Overfitting) ê°ì†Œ â†’ ì—¬ëŸ¬ ê°œì˜ ë‚˜ë¬´ê°€ í•™ìŠµí•˜ë¯€ë¡œ ë‹¨ì¼ íŠ¸ë¦¬ë³´ë‹¤ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ìŒ (More trees = Less overfitting)

Feature Importance: Identifies which variables are most predictive.

âŒ ë‹¨ì :

âš  ì†ë„ê°€ ëŠë¦¼ â†’ íŠ¸ë¦¬ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê³„ì‚°ëŸ‰ì´ ì¦ê°€

âš  í•´ì„í•˜ê¸° ì–´ë ¤ì›€ â†’ ê°œë³„ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ëŠ” í•´ì„ì´ ì‰¬ìš´ë°, ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ê°œì˜ íŠ¸ë¦¬ë¥¼ ì¡°í•©í•˜ë¯€ë¡œ í•´ì„ì´ ì–´ë ¤ì›€


ğŸ”¥ Key Takeaways for Interviews

<img width="744" alt="Screen Shot 2025-01-19 at 2 45 26 AM" src="https://github.com/user-attachments/assets/fb530ed2-fbd1-40f4-b3dc-8224af6bb185" />

âœ… Decision Trees â†’ Easy to interpret, but prone to overfitting.

âœ… CART for Regression â†’ Minimizes SSE.

âœ… CART for Classification â†’ Minimizes impurity using Gini or Cross-Entropy.

âœ… Random Forest â†’ Reduces variance using bootstrapping & bagging.

âœ… Adding More Trees â†’ Improves accuracy but does not cause overfitting.

âœ… Variable Importance â†’ Helps identify key features for decision-making.

âœ” ì˜ì‚¬ê²°ì •ë‚˜ë¬´ â†’ ë‹¨ìˆœí•˜ê³  ì§ê´€ì ì´ì§€ë§Œ ê³¼ì í•© ìœ„í—˜

âœ” ëœë¤ í¬ë ˆìŠ¤íŠ¸ â†’ ì—¬ëŸ¬ ê°œì˜ íŠ¸ë¦¬ë¥¼ ì¡°í•©í•´ì„œ ê³¼ì í•©ì„ ì¤„ì´ê³  ì„±ëŠ¥ í–¥ìƒ

âœ” Gini vs Entropy â†’ GiniëŠ” ì—°ì‚°ì´ ë¹ ë¥´ê³ , EntropyëŠ” ì •ë³´ì´ë¡  ê¸°ë°˜
