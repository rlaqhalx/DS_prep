# Decision Tree

# 📌 1. Classification and Regression Trees (CART)

Decision Trees are used for both classification and regression tasks. They work by splitting the dataset into smaller regions based on feature values.

🔹 How **CART** Works?

### For Regression:

- Splits data to minimize Sum of Squared Errors (SSE). 즉, 데이터를 여러 그룹으로 나누면서 SSE(오차 제곱합)를 최소화하는 방향으로 나눔

- Each leaf node represents the average of the target values in that region. 즉, 마지막 노드(Leaf Node)에서 평균값을 예측값으로 사용

- Complexity Parameter (cp) controls tree depth (small cp → deep tree).

📌 예제: 집값 예측

✔ 면적 > 50㎡ → 집값 평균 5000만 원

✔ 면적 ≤ 50㎡ → 집값 평균 3000만 원

✅ 중요한 개념:

✔ SSE(Sum of Squared Errors, 오차 제곱합) 최소화

✔ 깊은 트리(Deep Tree)는 과적합(Overfitting) 위험 있음 → 이를 방지하려면 Pruning(가지치기)나 Complexity Parameter(cp) 사용

### For Classification:

- Splits data to minimize impurity in each region. 즉, 데이터를 나누면서 불순도(Impurity) 를 최소화하는 방향으로 나눔

- Each leaf node assigns the most frequent class label. 마찬가지로 마지막 노드(Leaf Node)에서 가장 많은 클래스를 정답으로 예측

📌 예제: 이메일 스팸 분류

✔ 제목에 "Free" 포함 → 스팸 확률 80%

✔ 제목에 "Urgent" 포함 → 스팸 확률 70%

### Common impurity measures:

**Gini Impurity**: Measures the probability of misclassification.

Gini = 1 - Σ(p̂ᵢ²)

- 데이터가 특정 클래스로 얼마나 균일하게 분포되어 있는지 측정

- 값이 작을수록 더 순수한 그룹 (즉, 같은 클래스가 많음)

**Cross Entropy (Log Loss)**: Measures information gain from each split.

Cross Entropy = -Σ(p̂ᵢ log₂ p̂ᵢ)

- 데이터의 불확실성을 측정

- 값이 작을수록 더 예측하기 쉬운 데이터


📌 비교: 지니 불순도 vs 엔트로피

✔ Gini 불순도 → 연산 속도가 빠름

✔ Entropy → 정보이론 기반, 작은 데이터에서 조금 더 안정적


### Decision Tree 가 잘하는 것 vs 어려운 것

✔ 장점:

✅ 해석하기 쉽다 → 트리 구조만 보면 어떻게 예측하는지 직관적으로 이해 가능

✅ 이상치(Outlier)와 다중공선성(Multicollinearity)에 강함 → 트리가 스스로 중요한 변수를 선택 

(Multicollinearity & Outliers: Trees naturally handle them.)

❌ 단점:

⚠ 과적합(Overfitting) 위험 → 트리가 너무 깊어지면 훈련 데이터에는 잘 맞지만 새로운 데이터에는 성능이 떨어짐

⚠ 작은 변화에도 민감함 → 데이터가 조금만 변해도 트리 구조가 크게 달라질 수 있음 

(High Variance (Overfitting): Controlled using Cross Validation (CV) and pruning.)


# 📌 2. Random Forest (랜덤 포레스트)

An ensemble of multiple decision trees, reducing overfitting and improving accuracy.

랜덤 포레스트는 여러 개의 의사결정나무를 조합해 더 강력한 모델을 만드는 방법입니다.

📌 왜 랜덤 포레스트를 사용할까?

✔ 의사결정나무 하나만 사용하면 과적합될 가능성이 큼

✔ 랜덤 포레스트는 여러 개의 나무를 평균내서 예측 → 과적합을 방지하고 일반화 성능을 높임!


🔹 How Random Forest Works?

### 1. Bootstrapping: Randomly selects samples with replacement to train each tree.

훈련 데이터를 랜덤하게 뽑아서 여러 개의 의사결정나무를 훈련

즉, 각 나무는 다른 데이터 샘플을 학습함 → 다양성을 확보!

### 2. Bagging (Bootstrap Aggregation):

Each tree is trained on ~63% of the data.

The remaining 37% (out-of-bag data) helps estimate model performance without cross-validation.

각 트리(나무)는 랜덤하게 샘플링된 데이터로 훈련됨

결과를 평균 내거나 다수결 투표를 해서 최종 예측 결정!


![image](https://github.com/user-attachments/assets/7550b96d-371c-4136-a70e-d5fa0d832173)


Final Prediction:

Regression: Averages predictions from all trees.

Classification: Majority voting from all trees.

✅ Example:

Predicting whether a customer will buy a product based on past purchases.

Predicting loan approval based on credit score, income, and debt.

### Random Forest 가 잘하는 것 vs 어려운 것

✔ 장점:

✅ 비선형 데이터(Non-linearity)도 잘 처리

✅ 고차원 데이터(High-Dimensional Data)에도 강함

✅ 과적합(Overfitting) 감소 → 여러 개의 나무가 학습하므로 단일 트리보다 일반화 성능이 좋음 (More trees = Less overfitting)

Feature Importance: Identifies which variables are most predictive.

❌ 단점:

⚠ 속도가 느림 → 트리가 많아질수록 계산량이 증가

⚠ 해석하기 어려움 → 개별 의사결정나무는 해석이 쉬운데, 랜덤 포레스트는 여러 개의 트리를 조합하므로 해석이 어려움


🔥 Key Takeaways for Interviews

<img width="744" alt="Screen Shot 2025-01-19 at 2 45 26 AM" src="https://github.com/user-attachments/assets/fb530ed2-fbd1-40f4-b3dc-8224af6bb185" />

✅ Decision Trees → Easy to interpret, but prone to overfitting.

✅ CART for Regression → Minimizes SSE.

✅ CART for Classification → Minimizes impurity using Gini or Cross-Entropy.

✅ Random Forest → Reduces variance using bootstrapping & bagging.

✅ Adding More Trees → Improves accuracy but does not cause overfitting.

✅ Variable Importance → Helps identify key features for decision-making.

✔ 의사결정나무 → 단순하고 직관적이지만 과적합 위험

✔ 랜덤 포레스트 → 여러 개의 트리를 조합해서 과적합을 줄이고 성능 향상

✔ Gini vs Entropy → Gini는 연산이 빠르고, Entropy는 정보이론 기반
