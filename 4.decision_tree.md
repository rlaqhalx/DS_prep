# Decision Tree

# 📌 1. Classification and Regression Trees (CART)

Decision Trees are used for both classification and regression tasks. They work by splitting the dataset into smaller regions based on feature values.

🔹 How **CART** Works?

### For Regression:

- Splits data to minimize Sum of Squared Errors (SSE). 즉, 데이터를 여러 그룹으로 나누면서 SSE(오차 제곱합)를 최소화하는 방향으로 나눔

- Each leaf node represents the average of the target values in that region. 즉, 마지막 노드(Leaf Node)에서 평균값을 예측값으로 사용

- Complexity Parameter (cp) controls tree depth (small cp → deep tree).

📌 예제: 집값 예측

✔ 면적 > 50㎡ → 집값 평균 5000만 원

✔ 면적 ≤ 50㎡ → 집값 평균 3000만 원

✅ 중요한 개념:

✔ SSE(Sum of Squared Errors, 오차 제곱합) 최소화

✔ 깊은 트리(Deep Tree)는 과적합(Overfitting) 위험 있음 → 이를 방지하려면 Pruning(가지치기)나 Complexity Parameter(cp) 사용

### For Classification:

- Splits data to minimize impurity in each region. 즉, 데이터를 나누면서 불순도(Impurity) 를 최소화하는 방향으로 나눔

- Each leaf node assigns the most frequent class label. 마찬가지로 마지막 노드(Leaf Node)에서 가장 많은 클래스를 정답으로 예측

📌 예제: 이메일 스팸 분류

✔ 제목에 "Free" 포함 → 스팸 확률 80%

✔ 제목에 "Urgent" 포함 → 스팸 확률 70%

### Common impurity measures:

**Gini Impurity**: Measures the probability of misclassification.

Gini = 1 - Σ(p̂ᵢ²)

- 데이터가 특정 클래스로 얼마나 균일하게 분포되어 있는지 측정

- 값이 작을수록 더 순수한 그룹 (즉, 같은 클래스가 많음)

**Cross Entropy (Log Loss)**: Measures information gain from each split.

Cross Entropy = -Σ(p̂ᵢ log₂ p̂ᵢ)

- 데이터의 불확실성을 측정

- 값이 작을수록 더 예측하기 쉬운 데이터


📌 비교: 지니 불순도 vs 엔트로피

✔ Gini 불순도 → 연산 속도가 빠름

✔ Entropy → 정보이론 기반, 작은 데이터에서 조금 더 안정적


### Decision Tree 가 잘하는 것 vs 어려운 것

✔ 장점:

✅ 해석하기 쉽다 → 트리 구조만 보면 어떻게 예측하는지 직관적으로 이해 가능

✅ 이상치(Outlier)와 다중공선성(Multicollinearity)에 강함 → 트리가 스스로 중요한 변수를 선택 

(Multicollinearity & Outliers: Trees naturally handle them.)

❌ 단점:

⚠ 과적합(Overfitting) 위험 → 트리가 너무 깊어지면 훈련 데이터에는 잘 맞지만 새로운 데이터에는 성능이 떨어짐

⚠ 작은 변화에도 민감함 → 데이터가 조금만 변해도 트리 구조가 크게 달라질 수 있음 

(High Variance (Overfitting): Controlled using Cross Validation (CV) and pruning.)


# 📌 2. Random Forest (랜덤 포레스트)

An ensemble of multiple decision trees, reducing overfitting and improving accuracy.

랜덤 포레스트는 여러 개의 의사결정나무를 조합해 더 강력한 모델을 만드는 방법입니다.

📌 왜 랜덤 포레스트를 사용할까?

✔ 의사결정나무 하나만 사용하면 과적합될 가능성이 큼

✔ 랜덤 포레스트는 여러 개의 나무를 평균내서 예측 → 과적합을 방지하고 일반화 성능을 높임!


🔹 How Random Forest Works?

### 1. Bootstrapping: Randomly selects samples with replacement to train each tree.

훈련 데이터를 랜덤하게 뽑아서 여러 개의 의사결정나무를 훈련

즉, 각 나무는 다른 데이터 샘플을 학습함 → 다양성을 확보!

✔ 통계에서 사용하는 데이터 샘플링 기법

✔ 원래 데이터셋에서 랜덤하게 샘플을 뽑아 새로운 데이터셋을 만드는 방법

🔹 Bootstrapping의 특징

데이터를 중복을 허용하면서(Random Sampling with Replacement) 여러 번 샘플링함

즉, 한 데이터 포인트가 여러 번 선택될 수도 있음

다양한 데이터 샘플을 만들기 때문에 모델의 일반화 성능이 좋아짐

📌 예제:

만약 원본 데이터가 A, B, C, D, E라면, Bootstrapping으로 만든 샘플은 다음과 같을 수 있음:

✔ A, A, C, D, E

✔ B, C, C, D, E

✔ A, B, B, E, E

✔ 사용 목적: 데이터를 다양하게 사용하여 과적합을 방지하고 모델 성능을 높임

![image](https://github.com/user-attachments/assets/f0a896bd-b51f-4633-bfc6-d159ddd75673)


### 2. Bagging (Bootstrap Aggregation):

Each tree is trained on ~63% of the data.

The remaining 37% (out-of-bag data) helps estimate model performance without cross-validation.

각 트리(나무)는 랜덤하게 샘플링된 데이터로 훈련됨

결과를 평균 내거나 다수결 투표를 해서 최종 예측 결정!

✔ Bootstrapping을 활용한 앙상블 학습 방법

✔ 여러 개의 모델(예: 결정트리)을 독립적으로 학습한 후 평균 or 다수결 투표로 최종 예측

🔹 Bagging의 특징

1️⃣ 원본 데이터를 Bootstrapping으로 여러 개의 샘플을 생성

2️⃣ 각 샘플을 사용해 독립적인 모델(약한 학습기, Weak Learner) 학습

3️⃣ 최종 예측은 모든 모델의 평균(회귀) 또는 다수결(분류)로 결정

📌 Bagging의 핵심 아이디어:

여러 개의 모델을 독립적으로 훈련하여 과적합을 줄이고 성능을 향상

개별 모델들은 서로 다른 데이터를 학습하기 때문에 다양한 패턴을 학습

✅ 예제: 랜덤 포레스트(Random Forest)

✔ 랜덤 포레스트는 여러 개의 결정 트리를 만들고 배깅을 적용하여 최종 결과를 도출

✔ 즉, 각각의 결정 트리는 Bootstrapping을 통해 다른 샘플을 학습한 후, 최종적으로 다수결 투표로 결과를 결정


💡 Bagging이 좋은 이유?

✔ Overfitting(과적합)을 방지 → 여러 개의 모델을 조합해서 개별 모델이 너무 데이터에 집착하는 것을 방지

✔ 데이터 분산을 줄임 → 모델의 예측이 안정적

**Boosting & Boostrapping are DIFFERENT**

### 3. Boosting (부스팅)

✔ Bagging과 달리, 약한 모델(Weak Learner)을 순차적으로 학습시키는 방법

✔ 이전 모델이 잘못 예측한 데이터를 더 중요하게 학습하여 점점 더 정확한 모델을 만듦

🔹 Boosting의 특징

1️⃣ 첫 번째 모델을 학습하고 예측 오류를 계산

2️⃣ 잘못 예측한 데이터에 가중치를 더 부여하여 다음 모델이 더 신경 쓰도록 학습

3️⃣ 여러 개의 모델이 순차적으로 학습되면서 점점 더 좋은 성능을 냄

4️⃣ 최종 예측은 모든 모델의 가중합으로 결정

📌 Boosting의 핵심 아이디어:

이전 모델이 틀린 부분을 보완하면서 점점 더 강한 모델을 만들도록 학습

각 모델은 이전 모델보다 더 나은 성능을 내도록 개선됨

순차적 학습을 하기 때문에 Bagging보다 연산이 더 오래 걸림

✅ Boosting 알고리즘 종류 

✔ AdaBoost (Adaptive Boosting) → 간단한 부스팅 알고리즘 (ex: 약한 결정트리 조합)

✔ Gradient Boosting (GBM) → 강력한 성능을 내는 부스팅 방식

✔ XGBoost → GBM을 최적화한 버전 (캐글에서 많이 사용됨)

✔ LightGBM → 더 빠른 Boosting 알고리즘

💡 Boosting이 좋은 이유?

✔ 개별 모델이 점점 개선되면서 성능이 좋아짐

✔ Overfitting을 방지하면서도 높은 정확도를 달성

✔ 강력한 성능을 내는 머신러닝 알고리즘 중 하나!

![image](https://github.com/user-attachments/assets/7550b96d-371c-4136-a70e-d5fa0d832173)

즉:

✔ Bootstrapping → 데이터를 랜덤하게 뽑아 샘플링 (복원추출)

✔ Bagging → 여러 개의 모델을 독립적으로 학습 (병렬)

✔ Boosting → 이전 모델의 오류를 보완하며 점진적으로 학습 (순차적)

✔ Bagging = 과적합 방지, Boosting = 높은 성능

### Final Prediction:

Regression: Averages predictions from all trees.

Classification: Majority voting from all trees.

✅ Example:

Predicting whether a customer will buy a product based on past purchases.

Predicting loan approval based on credit score, income, and debt.

### Random Forest 가 잘하는 것 vs 어려운 것

✔ 장점:

✅ 비선형 데이터(Non-linearity)도 잘 처리

✅ 고차원 데이터(High-Dimensional Data)에도 강함

✅ 과적합(Overfitting) 감소 → 여러 개의 나무가 학습하므로 단일 트리보다 일반화 성능이 좋음 (More trees = Less overfitting)

Feature Importance: Identifies which variables are most predictive.

❌ 단점:

⚠ 속도가 느림 → 트리가 많아질수록 계산량이 증가

⚠ 해석하기 어려움 → 개별 의사결정나무는 해석이 쉬운데, 랜덤 포레스트는 여러 개의 트리를 조합하므로 해석이 어려움


🔥 Key Takeaways for Interviews

<img width="744" alt="Screen Shot 2025-01-19 at 2 45 26 AM" src="https://github.com/user-attachments/assets/fb530ed2-fbd1-40f4-b3dc-8224af6bb185" />

✅ Decision Trees → Easy to interpret, but prone to overfitting.

✅ CART for Regression → Minimizes SSE.

✅ CART for Classification → Minimizes impurity using Gini or Cross-Entropy.

✅ Random Forest → Reduces variance using bootstrapping & bagging.

✅ Adding More Trees → Improves accuracy but does not cause overfitting.

✅ Variable Importance → Helps identify key features for decision-making.

✔ 의사결정나무 → 단순하고 직관적이지만 과적합 위험

✔ 랜덤 포레스트 → 여러 개의 트리를 조합해서 과적합을 줄이고 성능 향상

✔ Gini vs Entropy → Gini는 연산이 빠르고, Entropy는 정보이론 기반
