# Support Vector Machines (SVM) & k-Nearest Neighbors (k-NN) 

# 📌 1. Support Vector Machines (SVM)

SVM is a supervised learning algorithm used for classification and regression tasks. It separates data points by finding the best hyperplane that maximizes the margin between two classes.

특징:

🔹 How SVM Works?

Finds the optimal hyperplane that best separates two classes. 즉, 데이터를 가장 잘 분리하는 **최적의 초평면(Hyperplane)** 을 찾음

Maximizes the margin between the decision boundary and the closest data points (called support vectors).  즉, **마진(Margin)** 을 최대화하여 일반화 성능을 높임

Uses different kernel functions to handle nonlinear classification problems. 비선형 데이터도 **커널 트릭(Kernel Trick)** 으로 변환 가능!


✅ Example:

✔ 분류(Classification) → 이메일이 스팸인지 아닌지

✔ 회귀(Regression) → 미래 주가 예측


## SVM이 작동하는 방식: SVM은 데이터를 가장 잘 나누는 초평면을 찾는 것이 목표!

📌 1️⃣ 초평면(Hyperplane) 찾기

2D 데이터에서는 직선, 3D에서는 평면, n차원에서는 초평면

SVM은 이 초평면을 통해 두 클래스를 최대한 떨어뜨리는 방향으로 학습

📌 2️⃣ 마진(Margin) 최대화

초평면과 가장 가까운 데이터 포인트(서포트 벡터) 사이의 거리를 최대화

마진이 넓을수록 일반화 성능이 좋아짐 (새로운 데이터에도 잘 적용됨)

📌 3️⃣ 서포트 벡터(Support Vectors)

초평면과 가장 가까운 데이터 포인트들

이 포인트들이 모델을 결정하는 중요한 요소!

## Support Vector Classifiers (SVC)

SVC extends SVM by handling outliers using the regularization parameter (C):

1️⃣ Regularization Parameter (C)

C는 모델이 얼마나 엄격하게 분류할지를 조정하는 정규화(Regularization) 파라미터입니다.

✔ C가 크면 (Strict, 낮은 오류 허용) → 초평면을 엄격하게 만듦, 오버피팅(Overfitting) 위험

✔ C가 작으면 (Flexible, 높은 오류 허용) → 일부 오류를 허용하지만 일반화 성능 향상

💡 Example:

✔ C가 너무 크면 [overfit] → 모든 데이터를 정확히 분류하지만 새로운 데이터에 약함 (strictly classifies data with fewer misclassifications less tolerant to outliers)

✔ C가 너무 작으면 [underfit] → 오류를 허용하면서도 일반화 성능이 좋아짐 (allows more misclassifications but improves generalization)


## Kernel Functions

For nonlinear problems, SVM uses kernel functions to map data into higher dimensions:

2️⃣ 커널 트릭 (Kernel Trick)

비선형 데이터를 처리하기 위해 SVM은 데이터를 고차원 공간으로 변환합니다.

이 변환을 수행하는 수학적 기법이 바로 커널(Kernel) 함수입니다.

✅ 자주 쓰이는 커널 함수들:
✔ 다항식 커널(Polynomial Kernel) → 곡선 형태의 경계를 만들 때 유용

✔ RBF 커널(Radial Basis Function Kernel) → 대부분의 데이터에서 강력한 성능

✔ 선형 커널(Linear Kernel) → 데이터가 선형적으로 분리될 수 있을 때 사용

📌 커널을 사용하면?

데이터가 곡선 형태로 분포해 있어도 SVM이 적절한 분리 경계를 만들 수 있음!

-------------------------------------------------------------------------------------------------------------------------------------------
1. Polynomial Kernel:

```(a · b + r)^d```

- d: Degree of the polynomial (higher d = more complex decision boundary).

- Useful for curved decision boundaries.


2. Radial Basis Function (RBF) Kernel:

```e^(-γ ||a - b||²)```

γ: Controls the influence of individual training samples.

- Smaller γ → Smoother decision boundary.

- Larger γ → More complex decision boundary (overfitting risk).

✅ Example:

RBF is used in image recognition, where relationships between pixel intensities are complex.

-------------------------------------------------------------------------------------------------------------------------------------------

## Hinge Loss Function: 데이터가 마진을 벗어나도록 분류되었을 때 패널티를 부여하는 방식

SVM minimizes the Hinge Loss, which penalizes misclassifications:

SVM은 Hinge Loss를 최소화하는 방식으로 학습합니다.

```Loss = max(0, 1 - yᵢ(wᵀxᵢ - b))```

- Even correctly classified points inside the margin still contribute to loss. (마진 안쪽에 있는 포인트들은 손실을 발생시킴)

- 마진을 벗어난 포인트들은 손실이 0

- Ensures better generalization.


✅ Example:

✔ 신용카드 사기 탐지 (실수하면 큰 비용 발생, 즉, 손실을 최소화해야 함)


## Multiclass Classification in SVM

Since SVM is inherently binary, multiclass classification is achieved through:

SVM은 기본적으로 이진 분류(Binary Classification) 모델이므로 다중 분류를 위해 **두 가지 기법** 을 사용합니다.

1️⃣ One-vs-Rest (OvR)

- Train a classifier for each class (positive) against all others (negative).

- Final prediction = Class with the highest confidence score.

2️⃣ One-vs-One (OvO)

- Train a classifier for each pair of classes.

- Final prediction = Class with the most wins.

✅ Example:

Handwritten Digit Recognition: Classifying numbers (0-9) using OvO.

# 📌 2. k-Nearest Neighbors (k-NN)

k-NN is a simple, non-parametric classification & regression algorithm. It predicts labels based on the closest k neighbors.

✔ 가장 가까운 데이터 포인트들을 보고 예측하는 간단한 알고리즘!

✔ 비선형 데이터에도 잘 작동하며, 학습 과정이 필요 없음

🔹 How k-NN Works?

1️⃣ Choose k (number of neighbors).

2️⃣ Find the k closest points to the query point.

3️⃣ Classification → Assigns the most common class among neighbors.

3️⃣ Regression → Predicts the average value of the neighbors.

✅ Example:

✔ Recommender Systems: Suggesting products based on customer similarity. 추천 시스템 (비슷한 고객끼리 추천)

✔ Health Monitoring: Predicting diseases based on similar patient data. 질병 예측 (비슷한 증상 환자 기반으로 예측)


## Distance Metrics in k-NN

k-NN calculates distance to find the closest points. Common metrics include:

1️⃣ Minkowski Distance (General form):

```(Σ |aᵢ - bᵢ|^p)^(1/p)```

- p = 1 → Manhattan Distance (absolute differences).

- p = 2 → Euclidean Distance (standard straight-line distance).

- p → ∞ (p가 매우 커지면) → 체비쇼프 거리 (Chebyshev Distance)
  
Minkowski Distance(민코프스키 거리)는 유클리드 거리(Euclidean Distance)와 맨해튼 거리(Manhattan Distance)의 일반화된 형태

<img width="746" alt="Screen Shot 2025-01-19 at 12 12 38 PM" src="https://github.com/user-attachments/assets/f94299a2-b4fc-490d-9c33-3bff72e9b03f" />

2️⃣ Hamming Distance:

- Counts the number of different elements in categorical data.

- Used in text processing & genetic sequence analysis.

✅ Example:

Manhattan Distance: Calculating taxi travel time in a grid-like city.

Hamming Distance: Comparing DNA sequences for genetic matching.

## 📌 SVM vs k-NN 차이점 정리

<img width="731" alt="Screen Shot 2025-01-19 at 12 15 28 PM" src="https://github.com/user-attachments/assets/aed55914-e308-4c42-9c31-077864548030" />

**언제 SVM vs k-NN을 사용할까?**

✔ 데이터가 선형적으로 분리 가능하다면? → SVM (고차원 가능)

✔ 데이터가 복잡하고 비선형 관계가 강하다면? → k-NN (가까운 데이터 기반 예측)

✔ 고차원 데이터에서 성능이 중요한 경우? → SVM

✔ 데이터가 적고 간단한 경우? → k-NN

**FINAL NOTE**

✔ SVM → 데이터를 초평면으로 나누는 강력한 분류 알고리즘

✔ k-NN → 가장 가까운 데이터 포인트를 보고 예측하는 간단한 알고리즘

✔ SVM = 일반화 성능 뛰어남, k-NN = 직관적이고 단순함

