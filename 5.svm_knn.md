# Support Vector Machines (SVM) & k-Nearest Neighbors (k-NN) 

# ğŸ“Œ 1. Support Vector Machines (SVM)

SVM is a supervised learning algorithm used for classification and regression tasks. It separates data points by finding the best hyperplane that maximizes the margin between two classes.

íŠ¹ì§•:

ğŸ”¹ How SVM Works?

Finds the optimal hyperplane that best separates two classes. ì¦‰, ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ë¶„ë¦¬í•˜ëŠ” **ìµœì ì˜ ì´ˆí‰ë©´(Hyperplane)** ì„ ì°¾ìŒ

Maximizes the margin between the decision boundary and the closest data points (called support vectors).  ì¦‰, **ë§ˆì§„(Margin)** ì„ ìµœëŒ€í™”í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì„

Uses different kernel functions to handle nonlinear classification problems. ë¹„ì„ í˜• ë°ì´í„°ë„ **ì»¤ë„ íŠ¸ë¦­(Kernel Trick)** ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥!


âœ… Example:

âœ” ë¶„ë¥˜(Classification) â†’ ì´ë©”ì¼ì´ ìŠ¤íŒ¸ì¸ì§€ ì•„ë‹Œì§€

âœ” íšŒê·€(Regression) â†’ ë¯¸ë˜ ì£¼ê°€ ì˜ˆì¸¡


## SVMì´ ì‘ë™í•˜ëŠ” ë°©ì‹: SVMì€ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ë‚˜ëˆ„ëŠ” ì´ˆí‰ë©´ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œ!

ğŸ“Œ 1ï¸âƒ£ ì´ˆí‰ë©´(Hyperplane) ì°¾ê¸°

2D ë°ì´í„°ì—ì„œëŠ” ì§ì„ , 3Dì—ì„œëŠ” í‰ë©´, nì°¨ì›ì—ì„œëŠ” ì´ˆí‰ë©´

SVMì€ ì´ ì´ˆí‰ë©´ì„ í†µí•´ ë‘ í´ë˜ìŠ¤ë¥¼ ìµœëŒ€í•œ ë–¨ì–´ëœ¨ë¦¬ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ

ğŸ“Œ 2ï¸âƒ£ ë§ˆì§„(Margin) ìµœëŒ€í™”

ì´ˆí‰ë©´ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸(ì„œí¬íŠ¸ ë²¡í„°) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”

ë§ˆì§„ì´ ë„“ì„ìˆ˜ë¡ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ì§ (ìƒˆë¡œìš´ ë°ì´í„°ì—ë„ ì˜ ì ìš©ë¨)

ğŸ“Œ 3ï¸âƒ£ ì„œí¬íŠ¸ ë²¡í„°(Support Vectors)

ì´ˆí‰ë©´ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸ë“¤

ì´ í¬ì¸íŠ¸ë“¤ì´ ëª¨ë¸ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œ!

## Support Vector Classifiers (SVC)

SVC extends SVM by handling outliers using the regularization parameter (C):

1ï¸âƒ£ Regularization Parameter (C)

CëŠ” ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì—„ê²©í•˜ê²Œ ë¶„ë¥˜í• ì§€ë¥¼ ì¡°ì •í•˜ëŠ” ì •ê·œí™”(Regularization) íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

âœ” Cê°€ í¬ë©´ (Strict, ë‚®ì€ ì˜¤ë¥˜ í—ˆìš©) â†’ ì´ˆí‰ë©´ì„ ì—„ê²©í•˜ê²Œ ë§Œë“¦, ì˜¤ë²„í”¼íŒ…(Overfitting) ìœ„í—˜

âœ” Cê°€ ì‘ìœ¼ë©´ (Flexible, ë†’ì€ ì˜¤ë¥˜ í—ˆìš©) â†’ ì¼ë¶€ ì˜¤ë¥˜ë¥¼ í—ˆìš©í•˜ì§€ë§Œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

ğŸ’¡ Example:

âœ” Cê°€ ë„ˆë¬´ í¬ë©´ [overfit] â†’ ëª¨ë“  ë°ì´í„°ë¥¼ ì •í™•íˆ ë¶„ë¥˜í•˜ì§€ë§Œ ìƒˆë¡œìš´ ë°ì´í„°ì— ì•½í•¨ (strictly classifies data with fewer misclassifications less tolerant to outliers)

âœ” Cê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ [underfit] â†’ ì˜¤ë¥˜ë¥¼ í—ˆìš©í•˜ë©´ì„œë„ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ì§ (allows more misclassifications but improves generalization)


## Kernel Functions

For nonlinear problems, SVM uses kernel functions to map data into higher dimensions:

2ï¸âƒ£ ì»¤ë„ íŠ¸ë¦­ (Kernel Trick)

ë¹„ì„ í˜• ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ SVMì€ ë°ì´í„°ë¥¼ ê³ ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ì´ ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” ìˆ˜í•™ì  ê¸°ë²•ì´ ë°”ë¡œ ì»¤ë„(Kernel) í•¨ìˆ˜ì…ë‹ˆë‹¤.

âœ… ìì£¼ ì“°ì´ëŠ” ì»¤ë„ í•¨ìˆ˜ë“¤:
âœ” ë‹¤í•­ì‹ ì»¤ë„(Polynomial Kernel) â†’ ê³¡ì„  í˜•íƒœì˜ ê²½ê³„ë¥¼ ë§Œë“¤ ë•Œ ìœ ìš©

âœ” RBF ì»¤ë„(Radial Basis Function Kernel) â†’ ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥

âœ” ì„ í˜• ì»¤ë„(Linear Kernel) â†’ ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¦¬ë  ìˆ˜ ìˆì„ ë•Œ ì‚¬ìš©

ğŸ“Œ ì»¤ë„ì„ ì‚¬ìš©í•˜ë©´?

ë°ì´í„°ê°€ ê³¡ì„  í˜•íƒœë¡œ ë¶„í¬í•´ ìˆì–´ë„ SVMì´ ì ì ˆí•œ ë¶„ë¦¬ ê²½ê³„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ!

-------------------------------------------------------------------------------------------------------------------------------------------
1. Polynomial Kernel:

```(a Â· b + r)^d```

- d: Degree of the polynomial (higher d = more complex decision boundary).

- Useful for curved decision boundaries.


2. Radial Basis Function (RBF) Kernel:

```e^(-Î³ ||a - b||Â²)```

Î³: Controls the influence of individual training samples.

- Smaller Î³ â†’ Smoother decision boundary.

- Larger Î³ â†’ More complex decision boundary (overfitting risk).

âœ… Example:

RBF is used in image recognition, where relationships between pixel intensities are complex.

-------------------------------------------------------------------------------------------------------------------------------------------

## Hinge Loss Function: ë°ì´í„°ê°€ ë§ˆì§„ì„ ë²—ì–´ë‚˜ë„ë¡ ë¶„ë¥˜ë˜ì—ˆì„ ë•Œ íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ì‹

SVM minimizes the Hinge Loss, which penalizes misclassifications:

SVMì€ Hinge Lossë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

```Loss = max(0, 1 - yáµ¢(wáµ€xáµ¢ - b))```

- Even correctly classified points inside the margin still contribute to loss. (ë§ˆì§„ ì•ˆìª½ì— ìˆëŠ” í¬ì¸íŠ¸ë“¤ì€ ì†ì‹¤ì„ ë°œìƒì‹œí‚´)

- ë§ˆì§„ì„ ë²—ì–´ë‚œ í¬ì¸íŠ¸ë“¤ì€ ì†ì‹¤ì´ 0

- Ensures better generalization.


âœ… Example:

âœ” ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ (ì‹¤ìˆ˜í•˜ë©´ í° ë¹„ìš© ë°œìƒ, ì¦‰, ì†ì‹¤ì„ ìµœì†Œí™”í•´ì•¼ í•¨)


## Multiclass Classification in SVM

Since SVM is inherently binary, multiclass classification is achieved through:

SVMì€ ê¸°ë³¸ì ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜(Binary Classification) ëª¨ë¸ì´ë¯€ë¡œ ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ìœ„í•´ **ë‘ ê°€ì§€ ê¸°ë²•** ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

1ï¸âƒ£ One-vs-Rest (OvR)

- Train a classifier for each class (positive) against all others (negative).

- Final prediction = Class with the highest confidence score.

2ï¸âƒ£ One-vs-One (OvO)

- Train a classifier for each pair of classes.

- Final prediction = Class with the most wins.

âœ… Example:

Handwritten Digit Recognition: Classifying numbers (0-9) using OvO.

# ğŸ“Œ 2. k-Nearest Neighbors (k-NN)

k-NN is a simple, non-parametric classification & regression algorithm. It predicts labels based on the closest k neighbors.

âœ” ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ ë³´ê³  ì˜ˆì¸¡í•˜ëŠ” ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜!

âœ” ë¹„ì„ í˜• ë°ì´í„°ì—ë„ ì˜ ì‘ë™í•˜ë©°, í•™ìŠµ ê³¼ì •ì´ í•„ìš” ì—†ìŒ

ğŸ”¹ How k-NN Works?

1ï¸âƒ£ Choose k (number of neighbors).

2ï¸âƒ£ Find the k closest points to the query point.

3ï¸âƒ£ Classification â†’ Assigns the most common class among neighbors.

3ï¸âƒ£ Regression â†’ Predicts the average value of the neighbors.

âœ… Example:

âœ” Recommender Systems: Suggesting products based on customer similarity. ì¶”ì²œ ì‹œìŠ¤í…œ (ë¹„ìŠ·í•œ ê³ ê°ë¼ë¦¬ ì¶”ì²œ)

âœ” Health Monitoring: Predicting diseases based on similar patient data. ì§ˆë³‘ ì˜ˆì¸¡ (ë¹„ìŠ·í•œ ì¦ìƒ í™˜ì ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡)


## Distance Metrics in k-NN

k-NN calculates distance to find the closest points. Common metrics include:

1ï¸âƒ£ Minkowski Distance (General form):

```(Î£ |aáµ¢ - báµ¢|^p)^(1/p)```

- p = 1 â†’ Manhattan Distance (absolute differences).

- p = 2 â†’ Euclidean Distance (standard straight-line distance).

- p â†’ âˆ (pê°€ ë§¤ìš° ì»¤ì§€ë©´) â†’ ì²´ë¹„ì‡¼í”„ ê±°ë¦¬ (Chebyshev Distance)
  
Minkowski Distance(ë¯¼ì½”í”„ìŠ¤í‚¤ ê±°ë¦¬)ëŠ” ìœ í´ë¦¬ë“œ ê±°ë¦¬(Euclidean Distance)ì™€ ë§¨í•´íŠ¼ ê±°ë¦¬(Manhattan Distance)ì˜ ì¼ë°˜í™”ëœ í˜•íƒœ

<img width="746" alt="Screen Shot 2025-01-19 at 12 12 38 PM" src="https://github.com/user-attachments/assets/f94299a2-b4fc-490d-9c33-3bff72e9b03f" />

2ï¸âƒ£ Hamming Distance:

- Counts the number of different elements in categorical data.

- Used in text processing & genetic sequence analysis.

âœ… Example:

Manhattan Distance: Calculating taxi travel time in a grid-like city.

Hamming Distance: Comparing DNA sequences for genetic matching.

## ğŸ“Œ SVM vs k-NN ì°¨ì´ì  ì •ë¦¬

<img width="731" alt="Screen Shot 2025-01-19 at 12 15 28 PM" src="https://github.com/user-attachments/assets/aed55914-e308-4c42-9c31-077864548030" />

**ì–¸ì œ SVM vs k-NNì„ ì‚¬ìš©í• ê¹Œ?**

âœ” ë°ì´í„°ê°€ ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¦¬ ê°€ëŠ¥í•˜ë‹¤ë©´? â†’ SVM (ê³ ì°¨ì› ê°€ëŠ¥)

âœ” ë°ì´í„°ê°€ ë³µì¡í•˜ê³  ë¹„ì„ í˜• ê´€ê³„ê°€ ê°•í•˜ë‹¤ë©´? â†’ k-NN (ê°€ê¹Œìš´ ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡)

âœ” ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ì„±ëŠ¥ì´ ì¤‘ìš”í•œ ê²½ìš°? â†’ SVM

âœ” ë°ì´í„°ê°€ ì ê³  ê°„ë‹¨í•œ ê²½ìš°? â†’ k-NN

**FINAL NOTE**

âœ” SVM â†’ ë°ì´í„°ë¥¼ ì´ˆí‰ë©´ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê°•ë ¥í•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜

âœ” k-NN â†’ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë³´ê³  ì˜ˆì¸¡í•˜ëŠ” ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜

âœ” SVM = ì¼ë°˜í™” ì„±ëŠ¥ ë›°ì–´ë‚¨, k-NN = ì§ê´€ì ì´ê³  ë‹¨ìˆœí•¨

