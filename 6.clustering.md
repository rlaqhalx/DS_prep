# Clustering

Clustering is an unsupervised machine learning technique that groups similar data points together based on distance or similarity measures.

클러스터링(Clustering)은 비지도 학습(Unsupervised Learning) 기법 중 하나로, 비슷한 데이터끼리 그룹을 형성하는 기법입니다.

✔ 고객 세분화(Customer Segmentation) → 유사한 소비 패턴을 가진 고객 그룹 찾기

✔ 이상 탐지(Anomaly Detection) → 정상 데이터와 이상 데이터를 구별

✔ 패턴 인식(Pattern Recognition) → 이미지 및 문서 그룹화

# 📌 1. k-Means Clustering

k-Means is a centroid-based clustering algorithm that assigns data points to k clusters, minimizing the variance within each cluster.

✔ 기본적인 중심 기반 클러스터링 알고리즘

✔ 데이터를 k개의 그룹으로 나누고, 각 그룹의 중심(centroid)을 찾음

✔ 데이터 포인트를 가장 가까운 중심에 할당하여 군집을 형성

<br />

🔹 How k-Means Works?

1. Randomly initialize k cluster centroids across the dataset.: k개의 중심(centroids) 랜덤 초기화

2. Assign each data point to the nearest centroid.: 각 데이터 포인트를 가장 가까운 중심에 할당

3. Recalculate centroids by computing the mean of all assigned points.: 할당된 데이터들의 평균을 계산하여 새로운 중심을 설정

4. Repeat until convergence, meaning centroids no longer change significantly.: 중심이 더 이상 변하지 않을 때까지 반복 (수렴)

<br />

✅ Example:

Customer Segmentation: Grouping customers based on purchasing behavior.

Image Compression: Clustering similar colors in an image for reduced storage.

<br />

### 💡 Robust Variations:

1️⃣ k-Medians: Uses the median instead of the mean, reducing sensitivity to outliers.

✔ 평균(mean) 대신 중앙값(median) 사용 → 이상치(Outliers)에 덜 민감

<br />

2️⃣ k-Medoids: Chooses actual data points as centroids, making it more robust to noise.

✔ 중심을 데이터셋 내의 실제 데이터 포인트로 선택

✔ 이상치와 노이즈에 강함

<br />

3️⃣ k-Modes: Used for categorical data, assigning clusters based on mode frequency.

✔ 범주형 데이터(categorical data)에 사용

✔ 최빈값(mode)을 기반으로 클러스터를 형성


✅ 예제:
✔ k-Medians → 금융 데이터 분석 (이상치 (outliers) 가 많은 경우)

✔ k-Modes → 설문조사 데이터에서 고객 취향 분석

<br />

🔵 k-Means++: Improved Initialization

k-Means++ improves cluster initialization, leading to better convergence and avoiding poor cluster placement.

💡 k-Means++는 기존 k-Means보다 더 효율적인 초기화 방법을 제공

<br />

🔹 Steps:

1. Pick the first centroid randomly.: 첫 번째 중심을 랜덤으로 선택

2. Calculate the distance between each point and the nearest centroid.

3. Select the next centroid using a weighted probability proportional to distance.: 나머지 중심을 데이터와 기존 중심 간 거리에 따라 확률적으로 선택

4. Repeat until k centroids are chosen.: k개의 중심이 선택될 때까지 반복

<br />

✅ Why use k-Means++?

Prevents clusters from starting too close together.

Leads to faster convergence and more stable clustering.

✔ 초기 중심이 잘못 설정되면 클러스터링이 비효율적이 될 수 있음

✔ k-Means++는 중심을 균형 있게 배치하여 더 빠르고 정확한 수렴 가능


✅ 예제:
✔ 고객 세분화 → 초기 클러스터 위치가 중요할 때

✔ 의료 데이터 분류 → 질병 유형별 그룹 형성


<img width="725" alt="Screen Shot 2025-01-20 at 2 31 05 AM" src="https://github.com/user-attachments/assets/99b31ff4-3339-402a-b631-3e2280c90ab7" />

<br />

🔵 Evaluating k-Means Clusters

To determine the optimal number of clusters (k), various methods can be used:

✅ 예제:
✔ 고객 세분화 → k=3과 k=4 중 최적의 k를 찾기 위해 엘보우 방법 사용

<br />

### 1️⃣ Silhouette Score:

Measures how well data points fit within their cluster vs. the next closest cluster.

Ranges from -1 to 1 → Closer to 1 means better clustering.

✔ 클러스터 내 데이터 간 거리를 평가

✔ 값 범위: -1 ~ 1 (1에 가까울수록 클러스터 품질이 좋음)

<br />

### 2️⃣ Davies-Bouldin Index:

**Ratio of within-cluster scatter to between-cluster** separation.

Lower values are better, indicating compact clusters with good separation.

✔ 클러스터 내 데이터 밀집도와 클러스터 간 거리 비율 측정

✔ 값이 낮을수록 좋은 클러스터링 (within-cluster 은 작고 between-cluster 은 크고 -> 작은 ratio)

<br />

### 3️⃣ Elbow Method:

Plots the within-cluster variance **(SSE)** for different values of k.

The optimal k is at the "elbow" where the SSE stops decreasing significantly.

✔ 각 k 값에 대한 클러스터 내 분산(SSE, Sum of Squared Errors) 측정

✔ SSE 감소가 급격히 줄어드는 지점이 최적의 k 값

<br />

✅ Example:

✔ 고객 세분화 → k=3과 k=4 중 최적의 k를 찾기 위해 엘보우 방법 사용


<img width="748" alt="Screen Shot 2025-01-20 at 2 31 33 AM" src="https://github.com/user-attachments/assets/03163cba-c38c-489a-8ee3-f5b5ee1e5dbf" />

<br />

# 📌 2. Hierarchical Clustering

Hierarchical Clustering builds a hierarchy of clusters, producing a tree-like structure (dendrogram).

✔ 트리 구조(덴드로그램, Dendrogram)를 이용하여 계층적으로 클러스터링

✔ 클러스터 개수(k)를 사전에 정하지 않아도 됨

<br />

🔹 Types of Hierarchical Clustering

### 1️⃣ Agglomerative Approach (Bottom-Up):

1. Each data point starts as its own cluster.

2. Merge the closest clusters iteratively.

3. Stops when all points are in one cluster.

### 2️⃣ Divisive Approach (Top-Down):

Starts with one large cluster containing all points.

Recursively splits clusters into smaller ones.

<br />

🔵 Linkage Methods in Hierarchical Clustering

계층적 클러스터링에서 두 개의 클러스터를 병합할 때 사용하는 방법

To determine how clusters should be merged, different linkage metrics are used:

**1️⃣ Single Linkage:**

- Merges clusters based on the closest pair of points.: 두 클러스터에서 가장 **가까운** 두 점 간 거리 사용

- Can result in "chain-like" clusters. -> 연쇄(cluster chaining) 문제 발생 가능 (긴 체인 같은 클러스터 형성)

**2️⃣ Complete Linkage:**

- Merges clusters based on the farthest pair of points.: 두 클러스터에서 가장 **먼** 두 점 간 거리 사용

- Creates more compact clusters. -> 더 균형 잡힌 클러스터 생성

**3️⃣ Ward’s Linkage:**

- Minimizes the increase in SSE (Sum of Squared Errors) when merging clusters.: 클러스터 내 분산(**SSE**)을 **최소화**하는 방식

- Produces balanced clusters. -> 보다 균형 잡힌 클러스터를 생성

✅ Example:

Genetic Similarity: Using Ward’s Linkage to cluster genetic sequences with minimal variance increase.

<br />


🔹 **Dendrograms** : Visualizing Hierarchical Clustering

계층적 클러스터링 결과를 시각적으로 표현한 트리 구조

✔ 노드의 높이가 클수록 클러스터 간 유사성이 낮음

✔ 특정 기준을 설정하여 최적의 클러스터 개수를 결정 가능

✅ Example:

Customer Relationship Analysis: Visualizing customer similarity using a dendrogram.

<img width="741" alt="Screen Shot 2025-01-20 at 2 32 07 AM" src="https://github.com/user-attachments/assets/973950df-e15c-4557-a37b-db74409734ee" />


<img width="728" alt="Screen Shot 2025-01-20 at 2 25 27 AM" src="https://github.com/user-attachments/assets/7f342ca1-af5c-48d1-9d20-2cc3688af34d" />

<br />

### FINAL NOTE

✔ k-Means → 빠르고 효율적이지만, K를 미리 정해야 함

✔ k-Means++ → 초기화 개선으로 더 나은 클러스터링

✔ 실루엣 점수 & 엘보우 방법 → 최적의 K 찾기

✔ 계층적 클러스터링 → 트리 구조로 데이터 관계 분석 가능

✔ Single vs. Complete vs. Ward’s Linkage → 목적에 맞는 병합 방법 선택

✔ 덴드로그램 → 클러스터링 결과를 시각화
