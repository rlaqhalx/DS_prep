# Clustering

Clustering is an unsupervised machine learning technique that groups similar data points together based on distance or similarity measures. It is commonly used for pattern recognition, customer segmentation, and anomaly detection.

# ðŸ“Œ 1. k-Means Clustering

k-Means is a centroid-based clustering algorithm that assigns data points to k clusters, minimizing the variance within each cluster.

ðŸ”¹ How k-Means Works?

1. Randomly initialize k cluster centroids across the dataset.

2. Assign each data point to the nearest centroid.

3. Recalculate centroids by computing the mean of all assigned points.

4. Repeat until convergence, meaning centroids no longer change significantly.

âœ… Example:

Customer Segmentation: Grouping customers based on purchasing behavior.

Image Compression: Clustering similar colors in an image for reduced storage.

ðŸ’¡ Robust Variations:

k-Medians: Uses the median instead of the mean, reducing sensitivity to outliers.

k-Medoids: Chooses actual data points as centroids, making it more robust to noise.

k-Modes: Used for categorical data, assigning clusters based on mode frequency.

ðŸ”µ k-Means++: Improved Initialization

k-Means++ improves cluster initialization, leading to better convergence and avoiding poor cluster placement.

ðŸ”¹ Steps:

1. Pick the first centroid randomly.

2. Calculate the distance between each point and the nearest centroid.

3. Select the next centroid using a weighted probability proportional to distance.

4. Repeat until k centroids are chosen.

âœ… Why use k-Means++?

Prevents clusters from starting too close together.

Leads to faster convergence and more stable clustering.

ðŸ”µ Evaluating k-Means Clusters

To determine the optimal number of clusters (k), various methods can be used:

### Silhouette Score:

Measures how well data points fit within their cluster vs. the next closest cluster.

Ranges from -1 to 1 â†’ Closer to 1 means better clustering.

### Davies-Bouldin Index:

Ratio of within-cluster scatter to between-cluster separation.

Lower values are better, indicating compact clusters with good separation.

### Elbow Method:

Plots the within-cluster variance (SSE) for different values of k.

The optimal k is at the "elbow" where the SSE stops decreasing significantly.

âœ… Example:

Choosing k for customer segmentation â†’ Using the Elbow Method to decide whether k=3 or k=4 is best.


# ðŸ“Œ 2. Hierarchical Clustering

Hierarchical Clustering builds a hierarchy of clusters, producing a tree-like structure (dendrogram).

ðŸ”¹ Types of Hierarchical Clustering

### Agglomerative Approach (Bottom-Up):

1. Each data point starts as its own cluster.

2. Merge the closest clusters iteratively.

3. Stops when all points are in one cluster.

### Divisive Approach (Top-Down):

Starts with one large cluster containing all points.

Recursively splits clusters into smaller ones.

âœ… Example:

Document Clustering: Grouping similar news articles together.

Genomic Data Analysis: Finding relationships between DNA sequences.

ðŸ”µ Linkage Methods in Hierarchical Clustering

To determine how clusters should be merged, different linkage metrics are used:

**Single Linkage:**

- Merges clusters based on the closest pair of points.

- Can result in "chain-like" clusters.

**Complete Linkage:**

- Merges clusters based on the farthest pair of points.

- Creates more compact clusters.

**Wardâ€™s Linkage:**

- Minimizes the increase in SSE (Sum of Squared Errors) when merging clusters.

- Produces balanced clusters.

âœ… Example:

Genetic Similarity: Using Wardâ€™s Linkage to cluster genetic sequences with minimal variance increase.

ðŸ”¹ **Dendrograms** : Visualizing Hierarchical Clustering

A dendrogram is a tree diagram showing cluster hierarchy.

The height of each node represents the dissimilarity between merged clusters.

âœ… Example:

Customer Relationship Analysis: Visualizing customer similarity using a dendrogram.
