# Clustering

Clustering is an unsupervised machine learning technique that groups similar data points together based on distance or similarity measures.

í´ëŸ¬ìŠ¤í„°ë§(Clustering)ì€ ë¹„ì§€ë„ í•™ìŠµ(Unsupervised Learning) ê¸°ë²• ì¤‘ í•˜ë‚˜ë¡œ, ë¹„ìŠ·í•œ ë°ì´í„°ë¼ë¦¬ ê·¸ë£¹ì„ í˜•ì„±í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

âœ” ê³ ê° ì„¸ë¶„í™”(Customer Segmentation) â†’ ìœ ì‚¬í•œ ì†Œë¹„ íŒ¨í„´ì„ ê°€ì§„ ê³ ê° ê·¸ë£¹ ì°¾ê¸°

âœ” ì´ìƒ íƒì§€(Anomaly Detection) â†’ ì •ìƒ ë°ì´í„°ì™€ ì´ìƒ ë°ì´í„°ë¥¼ êµ¬ë³„

âœ” íŒ¨í„´ ì¸ì‹(Pattern Recognition) â†’ ì´ë¯¸ì§€ ë° ë¬¸ì„œ ê·¸ë£¹í™”

# ğŸ“Œ 1. k-Means Clustering

k-Means is a centroid-based clustering algorithm that assigns data points to k clusters, minimizing the variance within each cluster.

âœ” ê¸°ë³¸ì ì¸ ì¤‘ì‹¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜

âœ” ë°ì´í„°ë¥¼ kê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° ê·¸ë£¹ì˜ ì¤‘ì‹¬(centroid)ì„ ì°¾ìŒ

âœ” ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì— í• ë‹¹í•˜ì—¬ êµ°ì§‘ì„ í˜•ì„±

<br />

ğŸ”¹ How k-Means Works?

1. Randomly initialize k cluster centroids across the dataset.: kê°œì˜ ì¤‘ì‹¬(centroids) ëœë¤ ì´ˆê¸°í™”

2. Assign each data point to the nearest centroid.: ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì— í• ë‹¹

3. Recalculate centroids by computing the mean of all assigned points.: í• ë‹¹ëœ ë°ì´í„°ë“¤ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ìƒˆë¡œìš´ ì¤‘ì‹¬ì„ ì„¤ì •

4. Repeat until convergence, meaning centroids no longer change significantly.: ì¤‘ì‹¬ì´ ë” ì´ìƒ ë³€í•˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µ (ìˆ˜ë ´)

<br />

âœ… Example:

Customer Segmentation: Grouping customers based on purchasing behavior.

Image Compression: Clustering similar colors in an image for reduced storage.

<br />

### ğŸ’¡ Robust Variations:

1ï¸âƒ£ k-Medians: Uses the median instead of the mean, reducing sensitivity to outliers.

âœ” í‰ê· (mean) ëŒ€ì‹  ì¤‘ì•™ê°’(median) ì‚¬ìš© â†’ ì´ìƒì¹˜(Outliers)ì— ëœ ë¯¼ê°

<br />

2ï¸âƒ£ k-Medoids: Chooses actual data points as centroids, making it more robust to noise.

âœ” ì¤‘ì‹¬ì„ ë°ì´í„°ì…‹ ë‚´ì˜ ì‹¤ì œ ë°ì´í„° í¬ì¸íŠ¸ë¡œ ì„ íƒ

âœ” ì´ìƒì¹˜ì™€ ë…¸ì´ì¦ˆì— ê°•í•¨

<br />

3ï¸âƒ£ k-Modes: Used for categorical data, assigning clusters based on mode frequency.

âœ” ë²”ì£¼í˜• ë°ì´í„°(categorical data)ì— ì‚¬ìš©

âœ” ìµœë¹ˆê°’(mode)ì„ ê¸°ë°˜ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±


âœ… ì˜ˆì œ:
âœ” k-Medians â†’ ê¸ˆìœµ ë°ì´í„° ë¶„ì„ (ì´ìƒì¹˜ (outliers) ê°€ ë§ì€ ê²½ìš°)

âœ” k-Modes â†’ ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„°ì—ì„œ ê³ ê° ì·¨í–¥ ë¶„ì„

<br />

ğŸ”µ k-Means++: Improved Initialization

k-Means++ improves cluster initialization, leading to better convergence and avoiding poor cluster placement.

ğŸ’¡ k-Means++ëŠ” ê¸°ì¡´ k-Meansë³´ë‹¤ ë” íš¨ìœ¨ì ì¸ ì´ˆê¸°í™” ë°©ë²•ì„ ì œê³µ

<br />

ğŸ”¹ Steps:

1. Pick the first centroid randomly.: ì²« ë²ˆì§¸ ì¤‘ì‹¬ì„ ëœë¤ìœ¼ë¡œ ì„ íƒ

2. Calculate the distance between each point and the nearest centroid.

3. Select the next centroid using a weighted probability proportional to distance.: ë‚˜ë¨¸ì§€ ì¤‘ì‹¬ì„ ë°ì´í„°ì™€ ê¸°ì¡´ ì¤‘ì‹¬ ê°„ ê±°ë¦¬ì— ë”°ë¼ í™•ë¥ ì ìœ¼ë¡œ ì„ íƒ

4. Repeat until k centroids are chosen.: kê°œì˜ ì¤‘ì‹¬ì´ ì„ íƒë  ë•Œê¹Œì§€ ë°˜ë³µ

<br />

âœ… Why use k-Means++?

Prevents clusters from starting too close together.

Leads to faster convergence and more stable clustering.

âœ” ì´ˆê¸° ì¤‘ì‹¬ì´ ì˜ëª» ì„¤ì •ë˜ë©´ í´ëŸ¬ìŠ¤í„°ë§ì´ ë¹„íš¨ìœ¨ì ì´ ë  ìˆ˜ ìˆìŒ

âœ” k-Means++ëŠ” ì¤‘ì‹¬ì„ ê· í˜• ìˆê²Œ ë°°ì¹˜í•˜ì—¬ ë” ë¹ ë¥´ê³  ì •í™•í•œ ìˆ˜ë ´ ê°€ëŠ¥


âœ… ì˜ˆì œ:
âœ” ê³ ê° ì„¸ë¶„í™” â†’ ì´ˆê¸° í´ëŸ¬ìŠ¤í„° ìœ„ì¹˜ê°€ ì¤‘ìš”í•  ë•Œ

âœ” ì˜ë£Œ ë°ì´í„° ë¶„ë¥˜ â†’ ì§ˆë³‘ ìœ í˜•ë³„ ê·¸ë£¹ í˜•ì„±


<img width="725" alt="Screen Shot 2025-01-20 at 2 31 05 AM" src="https://github.com/user-attachments/assets/99b31ff4-3339-402a-b631-3e2280c90ab7" />

<br />

ğŸ”µ Evaluating k-Means Clusters

To determine the optimal number of clusters (k), various methods can be used:

âœ… ì˜ˆì œ:
âœ” ê³ ê° ì„¸ë¶„í™” â†’ k=3ê³¼ k=4 ì¤‘ ìµœì ì˜ kë¥¼ ì°¾ê¸° ìœ„í•´ ì—˜ë³´ìš° ë°©ë²• ì‚¬ìš©

<br />

### 1ï¸âƒ£ Silhouette Score:

Measures how well data points fit within their cluster vs. the next closest cluster.

Ranges from -1 to 1 â†’ Closer to 1 means better clustering.

âœ” í´ëŸ¬ìŠ¤í„° ë‚´ ë°ì´í„° ê°„ ê±°ë¦¬ë¥¼ í‰ê°€

âœ” ê°’ ë²”ìœ„: -1 ~ 1 (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í´ëŸ¬ìŠ¤í„° í’ˆì§ˆì´ ì¢‹ìŒ)

<br />

### 2ï¸âƒ£ Davies-Bouldin Index:

**Ratio of within-cluster scatter to between-cluster** separation.

Lower values are better, indicating compact clusters with good separation.

âœ” í´ëŸ¬ìŠ¤í„° ë‚´ ë°ì´í„° ë°€ì§‘ë„ì™€ í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ë¹„ìœ¨ ì¸¡ì •

âœ” ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ í´ëŸ¬ìŠ¤í„°ë§ (within-cluster ì€ ì‘ê³  between-cluster ì€ í¬ê³  -> ì‘ì€ ratio)

<br />

### 3ï¸âƒ£ Elbow Method:

Plots the within-cluster variance **(SSE)** for different values of k.

The optimal k is at the "elbow" where the SSE stops decreasing significantly.

âœ” ê° k ê°’ì— ëŒ€í•œ í´ëŸ¬ìŠ¤í„° ë‚´ ë¶„ì‚°(SSE, Sum of Squared Errors) ì¸¡ì •

âœ” SSE ê°ì†Œê°€ ê¸‰ê²©íˆ ì¤„ì–´ë“œëŠ” ì§€ì ì´ ìµœì ì˜ k ê°’

<br />

âœ… Example:

âœ” ê³ ê° ì„¸ë¶„í™” â†’ k=3ê³¼ k=4 ì¤‘ ìµœì ì˜ kë¥¼ ì°¾ê¸° ìœ„í•´ ì—˜ë³´ìš° ë°©ë²• ì‚¬ìš©


<img width="748" alt="Screen Shot 2025-01-20 at 2 31 33 AM" src="https://github.com/user-attachments/assets/03163cba-c38c-489a-8ee3-f5b5ee1e5dbf" />

<br />

# ğŸ“Œ 2. Hierarchical Clustering

Hierarchical Clustering builds a hierarchy of clusters, producing a tree-like structure (dendrogram).

âœ” íŠ¸ë¦¬ êµ¬ì¡°(ë´ë“œë¡œê·¸ë¨, Dendrogram)ë¥¼ ì´ìš©í•˜ì—¬ ê³„ì¸µì ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§

âœ” í´ëŸ¬ìŠ¤í„° ê°œìˆ˜(k)ë¥¼ ì‚¬ì „ì— ì •í•˜ì§€ ì•Šì•„ë„ ë¨

<br />

ğŸ”¹ Types of Hierarchical Clustering

### 1ï¸âƒ£ Agglomerative Approach (Bottom-Up):

1. Each data point starts as its own cluster.

2. Merge the closest clusters iteratively.

3. Stops when all points are in one cluster.

### 2ï¸âƒ£ Divisive Approach (Top-Down):

Starts with one large cluster containing all points.

Recursively splits clusters into smaller ones.

<br />

ğŸ”µ Linkage Methods in Hierarchical Clustering

ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì—ì„œ ë‘ ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ë³‘í•©í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•

To determine how clusters should be merged, different linkage metrics are used:

**1ï¸âƒ£ Single Linkage:**

- Merges clusters based on the closest pair of points.: ë‘ í´ëŸ¬ìŠ¤í„°ì—ì„œ ê°€ì¥ **ê°€ê¹Œìš´** ë‘ ì  ê°„ ê±°ë¦¬ ì‚¬ìš©

- Can result in "chain-like" clusters. -> ì—°ì‡„(cluster chaining) ë¬¸ì œ ë°œìƒ ê°€ëŠ¥ (ê¸´ ì²´ì¸ ê°™ì€ í´ëŸ¬ìŠ¤í„° í˜•ì„±)

**2ï¸âƒ£ Complete Linkage:**

- Merges clusters based on the farthest pair of points.: ë‘ í´ëŸ¬ìŠ¤í„°ì—ì„œ ê°€ì¥ **ë¨¼** ë‘ ì  ê°„ ê±°ë¦¬ ì‚¬ìš©

- Creates more compact clusters. -> ë” ê· í˜• ì¡íŒ í´ëŸ¬ìŠ¤í„° ìƒì„±

**3ï¸âƒ£ Wardâ€™s Linkage:**

- Minimizes the increase in SSE (Sum of Squared Errors) when merging clusters.: í´ëŸ¬ìŠ¤í„° ë‚´ ë¶„ì‚°(**SSE**)ì„ **ìµœì†Œí™”**í•˜ëŠ” ë°©ì‹

- Produces balanced clusters. -> ë³´ë‹¤ ê· í˜• ì¡íŒ í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±

âœ… Example:

Genetic Similarity: Using Wardâ€™s Linkage to cluster genetic sequences with minimal variance increase.

<br />


ğŸ”¹ **Dendrograms** : Visualizing Hierarchical Clustering

ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•œ íŠ¸ë¦¬ êµ¬ì¡°

âœ” ë…¸ë“œì˜ ë†’ì´ê°€ í´ìˆ˜ë¡ í´ëŸ¬ìŠ¤í„° ê°„ ìœ ì‚¬ì„±ì´ ë‚®ìŒ

âœ” íŠ¹ì • ê¸°ì¤€ì„ ì„¤ì •í•˜ì—¬ ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ë¥¼ ê²°ì • ê°€ëŠ¥

âœ… Example:

Customer Relationship Analysis: Visualizing customer similarity using a dendrogram.

<img width="741" alt="Screen Shot 2025-01-20 at 2 32 07 AM" src="https://github.com/user-attachments/assets/973950df-e15c-4557-a37b-db74409734ee" />


<img width="728" alt="Screen Shot 2025-01-20 at 2 25 27 AM" src="https://github.com/user-attachments/assets/7f342ca1-af5c-48d1-9d20-2cc3688af34d" />

<br />

### FINAL NOTE

âœ” k-Means â†’ ë¹ ë¥´ê³  íš¨ìœ¨ì ì´ì§€ë§Œ, Kë¥¼ ë¯¸ë¦¬ ì •í•´ì•¼ í•¨

âœ” k-Means++ â†’ ì´ˆê¸°í™” ê°œì„ ìœ¼ë¡œ ë” ë‚˜ì€ í´ëŸ¬ìŠ¤í„°ë§

âœ” ì‹¤ë£¨ì—£ ì ìˆ˜ & ì—˜ë³´ìš° ë°©ë²• â†’ ìµœì ì˜ K ì°¾ê¸°

âœ” ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ â†’ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ë°ì´í„° ê´€ê³„ ë¶„ì„ ê°€ëŠ¥

âœ” Single vs. Complete vs. Wardâ€™s Linkage â†’ ëª©ì ì— ë§ëŠ” ë³‘í•© ë°©ë²• ì„ íƒ

âœ” ë´ë“œë¡œê·¸ë¨ â†’ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì‹œê°í™”
