# Linear & Logistic Regression

# ğŸ“Œ 1. Linear Regression (ì„ í˜• íšŒê·€)

Linear regression models linear relationships between a continuous response variable (Y) and explanatory variables (X).

## Ordinary Least Squares (OLS)

The goal is to find the best-fit line:

íšŒê·€ì„ (ì§ì„ )ì„ ì°¾ê¸° ìœ„í•´ **ì˜¤ì°¨(SSE, ì”ì°¨ ì œê³±í•©)** ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.


yÌ‚ = Î²Ì‚â‚€ + Î²XÌ‚ + Ïµ


ğŸ’¡ To minimize the Sum of Squared Errors (SSE), we estimate Î² using the formula:

ğŸ’¡ "ìµœì ì˜ ê¸°ìš¸ê¸°(Î²)ë¥¼ ì°¾ëŠ” ë°©ë²•"


Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€Y

ğŸ“Œ ì‰½ê²Œ ì´í•´í•˜ë©´?
ìš°ë¦¬ëŠ” ë°ì´í„°(X, Y)ë¥¼ ë³´ê³  **"ê°€ì¥ ì˜ ë§ëŠ” ì§ì„ "**ì„ ì°¾ê³  ì‹¶ì€ ê²ƒ!
ê·¸ë˜ì„œ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê¸°ìš¸ê¸°(Î²)ë¥¼ ì°¾ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.


âœ… Example:

Predicting house prices based on square footage.

Estimating student grades based on study hours.


## Assumptions of Linear Regression

1. Linearity - The relationship between X and Y is linear.
   
   Xì™€ Yì˜ ê´€ê³„ê°€ ì„ í˜•ì´ì–´ì•¼ í•¨

   Xê°€ ì¦ê°€í•˜ë©´ Yë„ ì¼ì •í•œ ë¹„ìœ¨ë¡œ ì¦ê°€ or ê°ì†Œí•´ì•¼ í•¨

2. Independent Observations - No correlation between observations.

   ë°ì´í„°ë“¤ì´ ì„œë¡œ ë…ë¦½ì ì´ì–´ì•¼ í•¨ (í•œ ë°ì´í„°ê°€ ë‹¤ë¥¸ ë°ì´í„°ì— ì˜í–¥ì„ ì£¼ë©´ ì•ˆ ë¨)

3. Homoscedasticity - Errors have constant variance.

   ì˜¤ì°¨ì˜ ë¶„í¬ê°€ ì¼ì •í•´ì•¼ í•¨

   ì¦‰, Xê°€ ì»¤ì ¸ë„ ì˜¤ì°¨(Ïµ)ì˜ í¬ê¸°ê°€ ì¼ì •í•´ì•¼ í•¨

4. Normality - Errors follow a normal distribution.

   ì˜¤ì°¨(Ïµ)ëŠ” ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¼ì•¼ í•¨

   ì‰½ê²Œ ë§í•´, ë°ì´í„°ê°€ íŠ¹ì • íŒ¨í„´ ì—†ì´ ëœë¤í•´ì•¼ í•¨

5. Low Multicollinearity - Predictors (X variables) should not be highly correlated.

   ë…ë¦½ ë³€ìˆ˜ë“¤(X1, X2, X3...)ì´ ì„œë¡œ ê°•í•˜ê²Œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ë©´ ì•ˆ ë¨

   ex) í‚¤(cm)ì™€ í‚¤(m)ëŠ” ì‚¬ì‹¤ìƒ ê°™ì€ ì •ë³´ â†’ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜

## Multicollinearity Check: Variance Inflation Factor - measures severity of the multicollinearity ##

ğŸ’¡ ë‹¤ì¤‘ê³µì„ ì„± (Multicollinearity) í•´ê²° ë°©ë²•?

âœ” VIF(Variance Inflation Factor, ë¶„ì‚° íŒ½ì°½ ê³„ìˆ˜) í™•ì¸!

Use Variance Inflation Factor (VIF): 

VIF = 1 / (1 - RÂ²)

If VIF > 10, multicollinearity is an issue.


## Regularization 

Prevents overfitting by adding a penalty term Î» to the cost function.

ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•˜ë©´ ì˜¤ë²„í”¼íŒ…(Overfitting)ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ.

ê·¸ë˜ì„œ ê·œì œ(Regularization) ê¸°ë²•ì„ ì‚¬ìš©í•´ ëª¨ë¸ì„ ë‹¨ìˆœí™”!

### LASSO Regression (L1 Regularization)

Î» Î£|Î²Ì‚| (ì ˆëŒ€ê°’ íŒ¨ë„í‹° ì¶”ê°€)

Shrinks some coefficients to zero (Feature Selection!). 

Useful for sparse models.

ì¤‘ìš”í•˜ì§€ ì•Šì€ ë³€ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ Feature Selection ê°€ëŠ¥!

ì¦‰, ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±°!

### Ridge Regression (L2 Regularization)

Î» Î£(Î²Ì‚)Â² (ì œê³± íŒ¨ë„í‹° ì¶”ê°€)

Shrinks coefficients towards zero, but does not eliminate them.

Reduces multicollinearity effects.

ëª¨ë“  ë³€ìˆ˜ì˜ ê³„ìˆ˜ë¥¼ ì‘ê²Œ ë§Œë“¦ (0ì€ ì•„ë‹˜)

multicollinearity í•´ê²°ì— íš¨ê³¼ì !

### Elastic Net (L1 + L2) LASSO + Ridge ê²°í•©

Combines LASSO & Ridge to balance feature selection and coefficient shrinkage.

Feature Selection + multicollinearity í•´ê²°ì„ ë™ì‹œì—!

âœ… Example:

LASSO: Choosing the most important factors affecting house prices.

Ridge: Handling correlated predictors like "height" and "weight" in BMI calculation.


ğŸ“Œ ì–´ë–¨ ë•Œ ì‚¬ìš©?

âœ” LASSO â†’ ì¤‘ìš” ë³€ìˆ˜ë§Œ ì„ íƒí•˜ê³  ì‹¶ì„ ë•Œ

âœ” Ridge â†’ ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆëŠ” ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ

âœ” Elastic Net â†’ ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•˜ê³  ì‹¶ì„ ë•Œ


# ğŸ“Œ 2. Logistic Regression (ë¡œì§€ìŠ¤í‹± íšŒê·€)

Used for binary classification problems (Yes/No, 0/1, Spam/Not Spam).

ì„ í˜• íšŒê·€ëŠ” ì—°ì† ê°’ì„ ì˜ˆì¸¡í•˜ì§€ë§Œ, ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ë¶„ë¥˜(Classification) ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

âœ” ì´ë©”ì¼ì´ ìŠ¤íŒ¸ì¸ì§€ ì•„ë‹Œì§€? (Spam/Not Spam, 0 or 1)

âœ” ê³ ê°ì´ ì œí’ˆì„ êµ¬ë§¤í• ì§€ ì•„ë‹ì§€? (Yes/No, 1 or 0)

### Sigmoid Function (ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜)

Transforms predictions into probabilities:

P(Y = 1) = 1 / (1 + e^-(Î²â‚€+Î²X))

Output is between 0 and 1, representing probability.

A threshold (Î±) is set to classify results (e.g., Î± = 0.5 â†’ Y=1 if P > 0.5, else Y=0).

ì¶œë ¥ê°’ì´ 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ ë¡œ ë³€í™˜ë¨

íŠ¹ì • ê¸°ì¤€ê°’(Î±) ì´ìƒì´ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ë¶„ë¥˜

âœ… Example:

Predicting if a customer will buy a product (Yes/No).

Identifying if an email is spam or not.

### Assumptions of Logistic Regression

1. Linear relationship between X and log-odds of Y.

   ì„ í˜• íšŒê·€ì²˜ëŸ¼ Xê°€ ì¦ê°€í•  ë•Œ log(odds)ê°€ ì¼ì •í•˜ê²Œ ì¦ê°€í•´ì•¼ í•¨

   ì¦‰, Xê°€ ì»¤ì§€ë©´ Y=1ì´ ë  í™•ë¥ ì´ ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•´ì•¼ í•¨

2. Independent observations.

3. Low multicollinearity between predictors. (check VIF!)

### Odds & Log-Odds (ìŠ¹ì‚°ë¹„ & ë¡œê·¸ ìŠ¹ì‚°ë¹„)

Odds(Y=1) = P(Y=1) / (1 - P(Y=1))

If odds = 2:1 â†’ There is a twice higher chance of Y=1 happening.

ì˜ˆ: "í•©ê²© í™•ë¥ ì´ 80%"ë¼ë©´ Odds = 0.8 / 0.2 = 4  ì¦‰, í•©ê²©í•  ê°€ëŠ¥ì„±ì´ ë¶ˆí•©ê²©ë³´ë‹¤ 4ë°° ë†’ìŒ

Coefficients affect the odds exponentially:

**âœ” Log-Odds(ë¡œê·¸ ìŠ¹ì‚°ë¹„):**

ì„ í˜• íšŒê·€ì²˜ëŸ¼ í•´ì„ ê°€ëŠ¥

Xê°€ 1 ì¦ê°€í•  ë•Œ â†’ Oddsê°€ e^Î²ë°° ì¦ê°€

One unit increase in X â†’ Odds multiply by e^Î²

âœ… Example:

If Î² = 0.7 for "years of experience", then one additional year increases the odds of getting a job offer by e^0.7 â‰ˆ 2x.
Î² = 0.7ì´ë©´ Xê°€ 1 ì¦ê°€í•  ë•Œ OddsëŠ” e^0.7 â‰ˆ 2ë°° ì¦ê°€  ì¦‰, ê²½ë ¥ì´ 1ë…„ ì¦ê°€í•˜ë©´ ì·¨ì—… í™•ë¥ ì´ 2ë°° ì¦ê°€! :((
