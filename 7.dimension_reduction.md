# Dimension Reduction 

High-dimensional data can suffer from the curse of dimensionality, which increases the risk of **overfitting** and reduces model performance. Dimension reduction techniques help by removing redundant features while preserving essential information.

💡 차원 축소(Dimension Reduction) 기술을 사용하면, 중요한 정보는 유지하면서 불필요한 변수(Feature)를 제거하여 성능을 향상할 수 있습니다.

# 📌 1. Principal Component Analysis (PCA)

PCA projects data onto orthogonal vectors (principal components) that maximize variance, effectively reducing the number of dimensions while preserving patterns in the data.

PCA는 데이터를 직교 벡터(Principal Components, 주성분) 위에 투영하여 차원을 줄이면서도 데이터의 변동성(Variance)을 최대한 유지하는 기법입니다.

🔹 How PCA Works?

📌 1️⃣ 데이터 표준화 (Standardization)

: Standardize the data so that each feature has a mean of 0 and variance of 1.

: 각 특성(feature)이 평균 0, 분산 1이 되도록 변환

📌 2️⃣ 공분산 행렬(Covariance Matrix) 계산

:Compute the covariance matrix to understand feature relationships.

: 변수 간의 상관관계를 분석하여 어떤 변수들이 강한 관계가 있는지 확인

📌 3️⃣ 고유값(Eigenvalues) & 고유벡터(Eigenvectors) 계산

: Find eigenvalues and eigenvectors using Singular Value Decomposition (SVD) or eigendecomposition.

: 특이값 분해(SVD, Singular Value Decomposition) 또는 고유분해(Eigendecomposition) 사용

📌 4️⃣ 주성분(Principal Components, PC) 정렬

: Rank principal components (PCs) by variance explained:

: Proportion of Variance Explained = ```λᵢ / Σλ```

- Higher-ranked PCs retain more variance. (고유값(λ)이 큰 주성분(PCs)이 데이터를 더 잘 설명)

- Select the top k principal components that retain sufficient variance. (상위 k개의 주성분을 선택하여 차원 축소)


✅ Example:

Facial Recognition: PCA reduces image pixel dimensions while keeping distinguishing features.

Stock Market Analysis: Extracts dominant trends from stock price movements.

<img width="741" alt="Screen Shot 2025-01-26 at 11 30 35 AM" src="https://github.com/user-attachments/assets/470645f0-2343-462e-ab6a-2d6b4cf0f1b8" />

💡 Key Notes:

- PCA assumes linear relationships between features.

- Number of principal components ≤ number of features (p).

- PCA explains variance in X, not necessarily in the target variable Y. (PCA는 X의 분산을 설명하는 것이지, Y(목표 변수)와의 관계를 보장하지 않음)
  
✔ 즉, PCA는 회귀나 분류를 위한 최적의 특성을 제공하는 것이 아니라 데이터의 변동성을 최대한 유지하는 방식으로 차원을 축소합니다.


🔵 Sparse PCA

Sparse PCA limits the number of non-zero values in principal components, making it robust to noise and improving interpretability.

Sparse PCA는 주성분 내에서 0이 아닌 값의 개수를 제한하여 노이즈를 줄이고 해석 가능성을 높이는 기법입니다.

✔ 원래 PCA는 모든 특성이 주성분에 영향을 줌, 하지만 Sparse PCA는 중요한 특성만 남김

✅ Example:

Used in genomics to identify key genes affecting diseases.

# 📌 2. Linear Discriminant Analysis (LDA)

LDA is a supervised technique that maximizes separation between classes while minimizing within-class variance.

LDA는 지도 학습(Supervised Learning) 기반 차원 축소 기법

**클래스 간 분산을 최대화**하면서, **클래스 내 분산을 최소화**하는 방식으로 작동

🔹 How LDA Works?

📌 1️⃣ 클래스별 평균과 분산 계산

:Compute the mean and variance of each independent variable for each class Cᵢ.

: 각 클래스(Cᵢ)의 평균(Mean)과 분산(Variance) 계산

📌 2️⃣ 클래스 내 분산(Within-Class Variance, σ²_w) & 클래스 간 분산(Between-Class Variance, σ²_b) 계산

: Calculate within-class (σ²_w) and between-class (σ²_b) variance.

: 클래스 간 차이를 가장 극대화할 수 있는 축을 찾음

📌 3️⃣ 변환 행렬(W) 계산

: Find the transformation matrix:

```W = (σ²_w)⁻¹ (σ²_b)```

which maximizes Fisher’s signal-to-noise ratio. (Fisher’s Signal-to-Noise Ratio를 최대로 만드는 방향으로 변환)

📌 4️⃣ 가장 높은 신호 대 잡음 비율을 가진 판별 컴포넌트 선택

: Rank discriminant components by their signal-to-noise ratio.


✅ Example:

Handwritten Digit Recognition: LDA improves digit classification from images.

Medical Diagnosis: Differentiates between healthy and diseased patients.

💡 Key Notes:

Maximum number of components = C - 1 (where C is the number of classes).

<img width="735" alt="Screen Shot 2025-01-26 at 11 34 47 AM" src="https://github.com/user-attachments/assets/fb461773-47b3-4f3e-860b-ce044faa7ee7" />

Assumes:

- Independent variables follow a normal distribution.

- Equal variance across classes (homoscedasticity).

- Low multicollinearity.

![image](https://github.com/user-attachments/assets/7f56a352-d3b6-4d99-be96-311b64a4f675)

# 📌 3. Factor Analysis

Factor Analysis represents data using latent factors rather than original variables.

✔ 데이터를 원본 변수 대신 숨겨진 요인(Latent Factors)으로 표현하는 기법

✔ 서로 상관관계가 있는 여러 변수들을 공통 요인(Factors) 으로 축소

🔹 How Factor Analysis Works?

📌 1️⃣ 데이터 모델링

: Express data in the form:

```X = Lf + ϵ```

where X is the observed data, L represents factor loadings, f represents latent factors, and ϵ is the noise.

✔ X → 원본 변수

✔ L → 요인 적재(Factor Loadings, 변수와 요인의 관계)

✔ f → 잠재 요인(Latent Factors)

✔ ϵ → 노이즈(Noise)

📌 2️⃣ 요인 적재(Factor Loadings) 추정

: Estimate factor loadings to determine how much each variable contributes to a factor.

: 어떤 변수가 어떤 요인과 얼마나 관련이 있는지 측정

📌 3️⃣ Scree Plot(스크리 플롯)으로 요인 개수 결정

: Use Scree Plot to determine the number of factors to retain.

: 고유값(Eigenvalues) 을 분석하여 적절한 요인 개수 선택

✅ Example:

Psychological Testing: Identifies underlying personality traits from questionnaire responses.

Economic Analysis: Extracts key macroeconomic factors affecting market trends.

🔵 Scree Plot: Choosing the Right Number of Factors

Plots eigenvalues of factors or principal components.

Elbow Method: The optimal number of factors is where eigenvalues level off (forming an "elbow").

✅ Example:

Selecting the best number of principal components for dimensionality reduction in image processing.


### 🔥 Key Takeaways for Interviews

✅ PCA → Best for unsupervised dimensionality reduction, preserves variance.
✅ Sparse PCA → Useful when interpretability is key (e.g., genomics).
✅ LDA → Best for supervised classification tasks, maximizes class separation.
✅ Factor Analysis → Identifies latent factors, common in psychology & finance.
✅ Scree Plot → Helps decide the right number of components.
✅ Curse of Dimensionality → Too many features reduce model performance.

<img width="734" alt="Screen Shot 2025-01-26 at 11 41 16 AM" src="https://github.com/user-attachments/assets/ac501012-249f-4499-b28b-5a75eff35ddb" />
<img width="731" alt="Screen Shot 2025-01-26 at 11 49 07 AM" src="https://github.com/user-attachments/assets/b2a414ed-4bd3-4588-9323-00ff220d0280" />


![image](https://github.com/user-attachments/assets/7fdf943a-1566-4396-9af5-f5eb459d3408)
