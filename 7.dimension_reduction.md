# Dimension Reduction 

High-dimensional data can suffer from the curse of dimensionality, which increases the risk of **overfitting** and reduces model performance. Dimension reduction techniques help by removing redundant features while preserving essential information.

ğŸ’¡ ì°¨ì› ì¶•ì†Œ(Dimension Reduction) ê¸°ìˆ ì„ ì‚¬ìš©í•˜ë©´, ì¤‘ìš”í•œ ì •ë³´ëŠ” ìœ ì§€í•˜ë©´ì„œ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜(Feature)ë¥¼ ì œê±°í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ğŸ“Œ 1. Principal Component Analysis (PCA)

PCA projects data onto orthogonal vectors (principal components) that maximize variance, effectively reducing the number of dimensions while preserving patterns in the data.

PCAëŠ” ë°ì´í„°ë¥¼ ì§êµ ë²¡í„°(Principal Components, ì£¼ì„±ë¶„) ìœ„ì— íˆ¬ì˜í•˜ì—¬ ì°¨ì›ì„ ì¤„ì´ë©´ì„œë„ ë°ì´í„°ì˜ ë³€ë™ì„±(Variance)ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

ğŸ”¹ How PCA Works?

ğŸ“Œ 1ï¸âƒ£ ë°ì´í„° í‘œì¤€í™” (Standardization)

: Standardize the data so that each feature has a mean of 0 and variance of 1.

: ê° íŠ¹ì„±(feature)ì´ í‰ê·  0, ë¶„ì‚° 1ì´ ë˜ë„ë¡ ë³€í™˜

ğŸ“Œ 2ï¸âƒ£ ê³µë¶„ì‚° í–‰ë ¬(Covariance Matrix) ê³„ì‚°

:Compute the covariance matrix to understand feature relationships.

: ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ ê°•í•œ ê´€ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸

ğŸ“Œ 3ï¸âƒ£ ê³ ìœ ê°’(Eigenvalues) & ê³ ìœ ë²¡í„°(Eigenvectors) ê³„ì‚°

: Find eigenvalues and eigenvectors using Singular Value Decomposition (SVD) or eigendecomposition.

: íŠ¹ì´ê°’ ë¶„í•´(SVD, Singular Value Decomposition) ë˜ëŠ” ê³ ìœ ë¶„í•´(Eigendecomposition) ì‚¬ìš©

ğŸ“Œ 4ï¸âƒ£ ì£¼ì„±ë¶„(Principal Components, PC) ì •ë ¬

: Rank principal components (PCs) by variance explained:

: Proportion of Variance Explained = ```Î»áµ¢ / Î£Î»```

- Higher-ranked PCs retain more variance. (ê³ ìœ ê°’(Î»)ì´ í° ì£¼ì„±ë¶„(PCs)ì´ ë°ì´í„°ë¥¼ ë” ì˜ ì„¤ëª…)

- Select the top k principal components that retain sufficient variance. (ìƒìœ„ kê°œì˜ ì£¼ì„±ë¶„ì„ ì„ íƒí•˜ì—¬ ì°¨ì› ì¶•ì†Œ)


âœ… Example:

Facial Recognition: PCA reduces image pixel dimensions while keeping distinguishing features.

Stock Market Analysis: Extracts dominant trends from stock price movements.

<img width="741" alt="Screen Shot 2025-01-26 at 11 30 35 AM" src="https://github.com/user-attachments/assets/470645f0-2343-462e-ab6a-2d6b4cf0f1b8" />

ğŸ’¡ Key Notes:

- PCA assumes linear relationships between features.

- Number of principal components â‰¤ number of features (p).

- PCA explains variance in X, not necessarily in the target variable Y. (PCAëŠ” Xì˜ ë¶„ì‚°ì„ ì„¤ëª…í•˜ëŠ” ê²ƒì´ì§€, Y(ëª©í‘œ ë³€ìˆ˜)ì™€ì˜ ê´€ê³„ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ)
  
âœ” ì¦‰, PCAëŠ” íšŒê·€ë‚˜ ë¶„ë¥˜ë¥¼ ìœ„í•œ ìµœì ì˜ íŠ¹ì„±ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë°ì´í„°ì˜ ë³€ë™ì„±ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œí•©ë‹ˆë‹¤.


ğŸ”µ Sparse PCA

Sparse PCA limits the number of non-zero values in principal components, making it robust to noise and improving interpretability.

Sparse PCAëŠ” ì£¼ì„±ë¶„ ë‚´ì—ì„œ 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜ë¥¼ ì œí•œí•˜ì—¬ ë…¸ì´ì¦ˆë¥¼ ì¤„ì´ê³  í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì´ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

âœ” ì›ë˜ PCAëŠ” ëª¨ë“  íŠ¹ì„±ì´ ì£¼ì„±ë¶„ì— ì˜í–¥ì„ ì¤Œ, í•˜ì§€ë§Œ Sparse PCAëŠ” ì¤‘ìš”í•œ íŠ¹ì„±ë§Œ ë‚¨ê¹€

âœ… Example:

Used in genomics to identify key genes affecting diseases.

# ğŸ“Œ 2. Linear Discriminant Analysis (LDA)

LDA is a supervised technique that maximizes separation between classes while minimizing within-class variance.

LDAëŠ” ì§€ë„ í•™ìŠµ(Supervised Learning) ê¸°ë°˜ ì°¨ì› ì¶•ì†Œ ê¸°ë²•

**í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì„ ìµœëŒ€í™”**í•˜ë©´ì„œ, **í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°ì„ ìµœì†Œí™”**í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™

ğŸ”¹ How LDA Works?

ğŸ“Œ 1ï¸âƒ£ í´ë˜ìŠ¤ë³„ í‰ê· ê³¼ ë¶„ì‚° ê³„ì‚°

:Compute the mean and variance of each independent variable for each class Cáµ¢.

: ê° í´ë˜ìŠ¤(Cáµ¢)ì˜ í‰ê· (Mean)ê³¼ ë¶„ì‚°(Variance) ê³„ì‚°

ğŸ“Œ 2ï¸âƒ£ í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°(Within-Class Variance, ÏƒÂ²_w) & í´ë˜ìŠ¤ ê°„ ë¶„ì‚°(Between-Class Variance, ÏƒÂ²_b) ê³„ì‚°

: Calculate within-class (ÏƒÂ²_w) and between-class (ÏƒÂ²_b) variance.

: í´ë˜ìŠ¤ ê°„ ì°¨ì´ë¥¼ ê°€ì¥ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì¶•ì„ ì°¾ìŒ

ğŸ“Œ 3ï¸âƒ£ ë³€í™˜ í–‰ë ¬(W) ê³„ì‚°

: Find the transformation matrix:

```W = (ÏƒÂ²_w)â»Â¹ (ÏƒÂ²_b)```

which maximizes Fisherâ€™s signal-to-noise ratio. (Fisherâ€™s Signal-to-Noise Ratioë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” ë°©í–¥ìœ¼ë¡œ ë³€í™˜)

ğŸ“Œ 4ï¸âƒ£ ê°€ì¥ ë†’ì€ ì‹ í˜¸ ëŒ€ ì¡ìŒ ë¹„ìœ¨ì„ ê°€ì§„ íŒë³„ ì»´í¬ë„ŒíŠ¸ ì„ íƒ

: Rank discriminant components by their signal-to-noise ratio.


âœ… Example:

Handwritten Digit Recognition: LDA improves digit classification from images.

Medical Diagnosis: Differentiates between healthy and diseased patients.

ğŸ’¡ Key Notes:

Maximum number of components = C - 1 (where C is the number of classes).

<img width="735" alt="Screen Shot 2025-01-26 at 11 34 47 AM" src="https://github.com/user-attachments/assets/fb461773-47b3-4f3e-860b-ce044faa7ee7" />

Assumes:

- Independent variables follow a normal distribution.

- Equal variance across classes (homoscedasticity).

- Low multicollinearity.

![image](https://github.com/user-attachments/assets/7f56a352-d3b6-4d99-be96-311b64a4f675)

# ğŸ“Œ 3. Factor Analysis

Factor Analysis represents data using latent factors rather than original variables.

âœ” ë°ì´í„°ë¥¼ ì›ë³¸ ë³€ìˆ˜ ëŒ€ì‹  ìˆ¨ê²¨ì§„ ìš”ì¸(Latent Factors)ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ë²•

âœ” ì„œë¡œ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ì—¬ëŸ¬ ë³€ìˆ˜ë“¤ì„ ê³µí†µ ìš”ì¸(Factors) ìœ¼ë¡œ ì¶•ì†Œ

ğŸ”¹ How Factor Analysis Works?

ğŸ“Œ 1ï¸âƒ£ ë°ì´í„° ëª¨ë¸ë§

: Express data in the form:

```X = Lf + Ïµ```

where X is the observed data, L represents factor loadings, f represents latent factors, and Ïµ is the noise.

âœ” X â†’ ì›ë³¸ ë³€ìˆ˜

âœ” L â†’ ìš”ì¸ ì ì¬(Factor Loadings, ë³€ìˆ˜ì™€ ìš”ì¸ì˜ ê´€ê³„)

âœ” f â†’ ì ì¬ ìš”ì¸(Latent Factors)

âœ” Ïµ â†’ ë…¸ì´ì¦ˆ(Noise)

ğŸ“Œ 2ï¸âƒ£ ìš”ì¸ ì ì¬(Factor Loadings) ì¶”ì •

: Estimate factor loadings to determine how much each variable contributes to a factor.

: ì–´ë–¤ ë³€ìˆ˜ê°€ ì–´ë–¤ ìš”ì¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì¸¡ì •

ğŸ“Œ 3ï¸âƒ£ Scree Plot(ìŠ¤í¬ë¦¬ í”Œë¡¯)ìœ¼ë¡œ ìš”ì¸ ê°œìˆ˜ ê²°ì •

: Use Scree Plot to determine the number of factors to retain.

: ê³ ìœ ê°’(Eigenvalues) ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ìš”ì¸ ê°œìˆ˜ ì„ íƒ

âœ… Example:

Psychological Testing: Identifies underlying personality traits from questionnaire responses.

Economic Analysis: Extracts key macroeconomic factors affecting market trends.

ğŸ”µ Scree Plot: Choosing the Right Number of Factors

Plots eigenvalues of factors or principal components.

Elbow Method: The optimal number of factors is where eigenvalues level off (forming an "elbow").

âœ… Example:

Selecting the best number of principal components for dimensionality reduction in image processing.


### ğŸ”¥ Key Takeaways for Interviews

âœ… PCA â†’ Best for unsupervised dimensionality reduction, preserves variance.
âœ… Sparse PCA â†’ Useful when interpretability is key (e.g., genomics).
âœ… LDA â†’ Best for supervised classification tasks, maximizes class separation.
âœ… Factor Analysis â†’ Identifies latent factors, common in psychology & finance.
âœ… Scree Plot â†’ Helps decide the right number of components.
âœ… Curse of Dimensionality â†’ Too many features reduce model performance.

<img width="734" alt="Screen Shot 2025-01-26 at 11 41 16 AM" src="https://github.com/user-attachments/assets/ac501012-249f-4499-b28b-5a75eff35ddb" />
<img width="731" alt="Screen Shot 2025-01-26 at 11 49 07 AM" src="https://github.com/user-attachments/assets/b2a414ed-4bd3-4588-9323-00ff220d0280" />


![image](https://github.com/user-attachments/assets/7fdf943a-1566-4396-9af5-f5eb459d3408)
