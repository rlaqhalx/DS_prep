# ğŸ“Œ Model Evaluation

## ğŸ“Œ Regression Metrics
Regression models are evaluated based on how well they predict continuous values. Below are key metrics:

íšŒê·€ ëª¨ë¸ì€ ì—°ì†ì ì¸ ê°’(ì˜ˆ: ì§‘ê°’, ì˜¨ë„, ë§¤ì¶œ)ì„ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ í‰ê°€

### 1) Mean Squared Error (MSE)

Measures the average squared difference between actual and predicted values. ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ ê°„ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ í‰ê· ì„ ë‚¸ ê°’

Penalizes large errors more than small errors. ì¦‰ ì˜¤ì°¨ê°€ í´ìˆ˜ë¡ ë” í° íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬

Formula: 

**MSE = (1/n) * Î£(yi - Å·i)Â²**

- yi: Actual value
- Å·i: Predicted value
- n: Number of data points

âœ… Example:

If actual values are [3, 5, 7] and predicted values are [2.5, 5.5, 6]:

MSE = [(3 - 2.5)Â² + (5 - 5.5)Â² + (7 - 6)Â²] / 3 = 0.5

í° ì˜¤ì°¨ì— ë” ë¯¼ê° â†’ ì´ìƒì¹˜(outlier)ê°€ ë§ë‹¤ë©´ RMSEë¥¼ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ

### 2) Sum of Squared Errors (SSE)

Measures the total squared differences between actual and predicted values.

ëª¨ë“  ì˜¤ì°¨ë¥¼ ì œê³±í•˜ì—¬ í•©í•œ ê°’ìœ¼ë¡œ, ëª¨ë¸ì˜ ì í•©ë„ë¥¼ í‰ê°€


Formula:

**SSE = Î£(yi - Å·i)Â²**

âœ… When to use it?

Useful for comparing models; a lower SSE indicates a better fit.

ì—¬ëŸ¬ ëª¨ë¸ì„ ë¹„êµí•  ë•Œ ì‚¬ìš©

ê°’ì´ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì´ ë” ì˜ ë§ìŒ


### 3) Total Sum of Squares (SST)

Measures total variability in the response variable.

ì‹¤ì œ ê°’ë“¤ì´ í‰ê·  ê°’ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ ìˆëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ


Formula:

**SST = Î£(yi - È³)Â²**

- È³: Mean of actual values


SSTëŠ” ëª¨ë¸ ì—†ì´ í‰ê· ë§Œìœ¼ë¡œ ì˜ˆì¸¡í•  ë•Œì˜ ì´ ì˜¤ì°¨

SSEê°€ ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë” ì˜ ì„¤ëª…í•¨

### 4) RÂ² (R-Squared)

Measures the proportion of variance explained by the model.

Higher values (closer to 1) indicate a better fit.

Negative RÂ² means the model performs worse than just predicting the mean.

ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ

1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ê³ , 0 ë˜ëŠ” ìŒìˆ˜ì´ë©´ ëª¨ë¸ì´ ì˜ë¯¸ ì—†ìŒ

Formula:

**RÂ² = 1 - (SSE / SST)**

âœ… Example:

If SSE = 10 and SST = 50, then:

RÂ² = 1 - (10 / 50) = 0.8 (80% variability explained)


íšŒê·€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•  ë•Œ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë¸ì´ ë” ì •í™•í•¨

í•˜ì§€ë§Œ ë³€ìˆ˜ê°€ ë§ìœ¼ë©´ ì¡°ì •ëœ RÂ² ì‚¬ìš© í•„ìš”

### 5) Adjusted RÂ²

Adjusts RÂ² for the number of predictors.

Prevents overfitting when adding irrelevant variables.

ë³€ìˆ˜ë¥¼ ì¶”ê°€í•  ë•Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì •ë§ ì¢‹ì•„ì¡ŒëŠ”ì§€ í™•ì¸í•˜ëŠ” ì§€í‘œ

ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ê°€ ì¶”ê°€ë  ê²½ìš° RÂ²ì´ ë†’ì•„ì§€ëŠ” ë¬¸ì œë¥¼ ë°©ì§€

Formula:

**Adjusted RÂ² = 1 - (1 - RÂ²) * (N-1) / (N-p-1)**

- N: Number of observations
- p: Number of predictors

âœ… When to use it?

When comparing models with different numbers of features.

ì—¬ëŸ¬ ê°œì˜ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” íšŒê·€ ëª¨ë¸ ë¹„êµ

ë³€ìˆ˜ê°€ ë§ì„ ë•Œ ê³¼ì í•©(Overfitting)ì„ ë°©ì§€

## ğŸ“Œ Classification Metrics

Classification models are evaluated based on how well they predict categorical labels.

ë¶„ë¥˜ ëª¨ë¸ì€ ë²”ì£¼í˜•(ì´ì§„ ë˜ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤) ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í•˜ëŠ”ì§€ í‰ê°€


### 1) Confusion Matrix

A table summarizing model performance:

<img width="598" alt="Screen Shot 2025-01-17 at 3 14 50 AM" src="https://github.com/user-attachments/assets/849e6f28-73f1-42f7-bc3b-4cbaa520d952" />

âœ… Example:

If a spam detector predicts 10 spam emails correctly (TP), misses 5 spam emails (FN), mistakenly labels 2 non-spam as spam (FP), and correctly identifies 83 non-spam emails (TN):

<img width="594" alt="Screen Shot 2025-01-17 at 3 15 21 AM" src="https://github.com/user-attachments/assets/ed258824-d5b1-47ea-8a6a-ff616c87c5ea" />


### 2) Precision

Measures the proportion of correctly predicted positive instances.

Also known as Positive Predictive Value (PPV).

Formula:

**Precision = TP / (TP + FP)**

âœ… Example:

Precision = 10 / (10 + 2) = 0.83 (83%)

âœ… ì–¸ì œ ì‚¬ìš©?

FPë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ê²½ìš° (ì˜ˆ: ìŠ¤íŒ¸ í•„í„°, ì•” ì§„ë‹¨)


### 3) Recall (Sensitivity, True Positive Rate)

Measures how many actual positives were correctly identified.

Formula:

**Recall = TP / (TP + FN)**

âœ… Example:

Recall = 10 / (10 + 5) = 0.67 (67%)

âœ… ì–¸ì œ ì‚¬ìš©?

FNì„ ì¤„ì´ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ê²½ìš° (ì˜ˆ: ì•” ì§„ë‹¨, ì‚¬ê¸° íƒì§€)

### 4) Specificity (True Negative Rate)

Measures how many actual negatives were correctly identified.

Formula:

**Specificity = TN / (TN + FP)**

âœ… Example:

Specificity = 83 / (83 + 2) = 0.98 (98%)

### 5) F1-Score

Harmonic mean of precision and recall.

Useful when classes are imbalanced.

ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê· 

ë¶ˆê· í˜•í•œ ë°ì´í„°ì…‹ì—ì„œ ìœ ìš©í•¨

Formula:

**F1 = 2 * (Precision * Recall) / (Precision + Recall)**

âœ… Example:

F1 = 2 * (0.83 * 0.67) / (0.83 + 0.67) = 0.74 (74%)

### 6) ROC Curve & AUC

ëª¨ë¸ì˜ ì „ì²´ì ì¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê·¸ë˜í”„

AUC (ë©´ì )ê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ì¢‹ì€ ëª¨ë¸

ROC Curve: Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) for different thresholds.

AUC (Area Under Curve): Measures the ability to distinguish between positive and negative classes.

AUC = 1.0: Perfect model.

AUC = 0.5: No predictive power (random guessing).

âœ… Example:

If a model has an AUC score of 0.85, it means it correctly ranks a randomly chosen positive case higher than a randomly chosen negative case 85% of the time.

âœ… ì–¸ì œ ì‚¬ìš©?

ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ ë¹„êµ

### 7) Precision-Recall Curve

Focuses on the correct prediction of the minority class.

More useful than ROC when data is imbalanced (e.g., fraud detection, rare disease prediction).

âœ… Example:

If fraud detection has a 1% fraud rate, using Precision-Recall Curve is more informative than an ROC curve.


<img width="592" alt="Screen Shot 2025-01-17 at 3 17 35 AM" src="https://github.com/user-attachments/assets/94766191-693c-472b-8e77-7a9c9c995c36" />

âœ… MSE, RÂ² â†’ íšŒê·€ ëª¨ë¸ í‰ê°€

âœ… ì •ë°€ë„ (Precision) â†’ FP ì¤„ì´ëŠ” ë° ì¤‘ìš” (ìŠ¤íŒ¸ í•„í„°)

âœ… ì¬í˜„ìœ¨ (Recall) â†’ FN ì¤„ì´ëŠ” ë° ì¤‘ìš” (ì•” ì§„ë‹¨)

âœ… F1-ìŠ¤ì½”ì–´ â†’ ì •ë°€ë„ vs ì¬í˜„ìœ¨ ê· í˜• (ì‚¬ê¸° íƒì§€)

âœ… AUC â†’ ì „ì²´ì ì¸ ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€
